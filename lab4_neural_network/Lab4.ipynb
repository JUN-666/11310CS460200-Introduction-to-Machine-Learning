{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Neural Networks - Dense Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math # For math.floor\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "from matplotlib.animation import FuncAnimation # For animation helper (stubbed)\n",
    "import pandas as pd # For creating submission CSVs\n",
    "from sklearn.metrics import f1_score # For F1 score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dense Layer Class\n",
    "\n",
    "This section implements a fully connected (dense) layer for a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"Represents a dense (fully connected) layer of a neural network.\n",
    "\n",
    "    Attributes:\n",
    "        n_x (int): Number of input units (size of the previous layer).\n",
    "        n_y (int): Number of output units (size of this layer).\n",
    "        seed (int): Random seed for weight initialization for reproducibility.\n",
    "        parameters (dict): Dictionary containing the weights 'W' and biases 'b'.\n",
    "                         'W' has shape (n_x, n_y).\n",
    "                         'b' has shape (1, n_y).\n",
    "        cache (tuple): Stores values (A_prev, W, b) from the forward pass for use in backward pass.\n",
    "        dW (np.ndarray): Gradient of the loss with respect to W.\n",
    "        db (np.ndarray): Gradient of the loss with respect to b.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        \"\"\"Initializes the Dense layer.\n",
    "\n",
    "        Args:\n",
    "            n_x (int): Number of input units (features from the previous layer).\n",
    "            n_y (int): Number of output units (neurons in this layer).\n",
    "            seed (int, optional): Random seed for weight initialization. Defaults to 1.\n",
    "        \"\"\"\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.parameters = {}\n",
    "        self.cache = ()\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initializes the weights 'W' and biases 'b' for the layer.\n",
    "        \n",
    "        Uses Glorot/Xavier uniform initialization for weights.\n",
    "        Weights 'W' will have shape (n_x, n_y).\n",
    "        Biases 'b' will have shape (1, n_y).\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        limit = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        W = np.random.uniform(-limit, limit, (self.n_y, self.n_x)).T\n",
    "        b = np.zeros((1, self.n_y))\n",
    "        \n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"Performs the forward propagation step for the dense layer.\n",
    "\n",
    "        Computes Z = A @ W + b.\n",
    "\n",
    "        Args:\n",
    "            A (np.ndarray): Activations from the previous layer (or input data).\n",
    "                          Shape: (n_samples, n_x) where n_x is self.n_x.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The linear part of the activation (Z).\n",
    "                        Shape: (n_samples, n_y) where n_y is self.n_y.\n",
    "        \"\"\"\n",
    "        if A.ndim == 1:\n",
    "            if self.n_x == 1: \n",
    "                 A_proc = A.reshape(-1, 1)\n",
    "            else: \n",
    "                 A_proc = A.reshape(1, -1)\n",
    "        else:\n",
    "            A_proc = A\n",
    "\n",
    "        if A_proc.shape[1] != self.n_x:\n",
    "            raise ValueError(f\"Input A has {A_proc.shape[1]} features, but layer expects {self.n_x}.\")\n",
    "\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        \n",
    "        Z = A_proc @ W + b\n",
    "        self.cache = (A_proc, W, b) \n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"Performs the backward propagation step for the dense layer.\n",
    "\n",
    "        Computes the gradients dW, db, and dA_prev.\n",
    "        dW = (1/m) * A_prev.T @ dZ\n",
    "        db = (1/m) * sum(dZ, axis=0, keepdims=True)\n",
    "        dA_prev = dZ @ W.T\n",
    "\n",
    "        Args:\n",
    "            dZ (np.ndarray): Gradient of the cost with respect to the linear output Z of this layer.\n",
    "                             Shape: (n_samples, n_y).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the cost with respect to the activation of the previous layer (A_prev).\n",
    "                        Shape: (n_samples, n_x).\n",
    "        \"\"\"\n",
    "        A_prev, W, b = self.cache # A_prev is the input A to this layer from forward pass\n",
    "        \n",
    "        if A_prev is None or W is None or b is None or not hasattr(A_prev, 'size') or A_prev.size == 0:\n",
    "            raise ValueError(\"Cache is empty or invalid. Ensure forward pass is done before backward pass.\")\n",
    "        \n",
    "        m = A_prev.shape[0] # Number of samples\n",
    "        \n",
    "        if m == 0:\n",
    "            self.dW = np.zeros_like(W)\n",
    "            self.db = np.zeros_like(b)\n",
    "            dA_prev = np.zeros_like(A_prev)\n",
    "            return dA_prev\n",
    "\n",
    "        self.dW = (1/m) * (A_prev.T @ dZ)\n",
    "        self.db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "        dA_prev = dZ @ W.T\n",
    "        \n",
    "        assert self.dW.shape == W.shape, f\"dW shape mismatch: {self.dW.shape} vs {W.shape}\"\n",
    "        assert self.db.shape == b.shape, f\"db shape mismatch: {self.db.shape} vs {b.shape}\"\n",
    "        assert dA_prev.shape == A_prev.shape, f\"dA_prev shape mismatch: {dA_prev.shape} vs {A_prev.shape}\"\n",
    "        \n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"Updates the layer's parameters (W and b) using computed gradients.\n",
    "\n",
    "        Update rule: parameter = parameter - learning_rate * gradient\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): The learning rate for the update.\n",
    "        \"\"\"\n",
    "        if self.dW is None or self.db is None:\n",
    "            print(\"Warning: Gradients dW or db not computed. Call backward() first before updating parameters.\")\n",
    "            return\n",
    "        \n",
    "        self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "        self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for Dense Layer (Forward, Backward, and Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: 3 input features, 1 output feature\n",
    "# print(\"--- Example 1: Dense(n_x=3, n_y=1) ---\")\n",
    "dense_layer_ex1 = Dense(n_x=3, n_y=1, seed=42)\n",
    "A_sample_ex1 = np.array([[1, 2, 3], [4, 5, 6]], dtype=float)\n",
    "Z_output_ex1 = dense_layer_ex1.forward(A_sample_ex1)\n",
    "dZ_sample_ex1 = np.array([[0.5], [-0.2]])\n",
    "dA_prev_output_ex1 = dense_layer_ex1.backward(dZ_sample_ex1)\n",
    "learning_rate_ex = 0.1\n",
    "W_before_update_ex1 = dense_layer_ex1.parameters[\"W\"].copy()\n",
    "dense_layer_ex1.update(learning_rate_ex)\n",
    "# print(f\"W before update was different from W after update: {not np.array_equal(W_before_update_ex1, dense_layer_ex1.parameters['W'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Layer Class\n",
    "\n",
    "This class implements various activation functions used in neural networks. The `forward` method applies the chosen activation, and the `backward` method will compute its gradient with respect to its input Z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"Represents an activation layer of a neural network.\n",
    "\n",
    "    Attributes:\n",
    "        activation_function (str): The name of the activation function to use \n",
    "                                   ('sigmoid', 'relu', 'softmax', 'linear').\n",
    "        loss_function (str, optional): The name of the loss function associated with this activation,\n",
    "                                       relevant for some gradient calculations in the backward pass \n",
    "                                       (e.g. 'softmax' with 'cross_entropy').\n",
    "        cache (np.ndarray): Stores the input Z from the forward pass for use in the backward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function, loss_function=None):\n",
    "        \"\"\"Initializes the Activation layer.\n",
    "\n",
    "        Args:\n",
    "            activation_function (str): Type of activation ('sigmoid', 'relu', 'softmax', 'linear').\n",
    "            loss_function (str, optional): Type of loss function, used by some backward methods.\n",
    "                                           Defaults to None.\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.cache = None # Will store Z during forward pass\n",
    "\n",
    "    def forward(self, Z):\n",
    "        \"\"\"Performs the forward propagation step for the activation function.\n",
    "\n",
    "        Args:\n",
    "            Z (np.ndarray): The linear output from the previous (dense) layer.\n",
    "                          Shape can be (n_samples, n_units).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The activated output A.\n",
    "        \"\"\"\n",
    "        self.cache = Z \n",
    "        A = None\n",
    "\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            A = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n",
    "        elif self.activation_function == \"relu\":\n",
    "            A = np.maximum(0, Z)\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            if Z.ndim == 1: Z_reshaped = Z.reshape(1, -1)\n",
    "            else: Z_reshaped = Z\n",
    "            b = np.max(Z_reshaped, axis=1, keepdims=True) \n",
    "            exp_Z_shifted = np.exp(Z_reshaped - b)\n",
    "            A = exp_Z_shifted / np.sum(exp_Z_shifted, axis=1, keepdims=True)\n",
    "            if Z.ndim == 1: A = A.flatten()\n",
    "        elif self.activation_function == \"linear\":\n",
    "            A = Z.copy() \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        \"\"\"Performs the backward propagation step for the activation function.\n",
    "\n",
    "        Calculates dZ, the gradient of the cost with respect to Z (input of activation).\n",
    "\n",
    "        Args:\n",
    "            dA (np.ndarray, optional): Gradient of the cost with respect to the output A of this activation layer.\n",
    "                                     Required for 'sigmoid', 'relu', 'linear'. Shape should match A.\n",
    "            Y (np.ndarray, optional): True labels (one-hot encoded for softmax with CCE).\n",
    "                                     Required for 'softmax' when used with 'cross_entropy' loss.\n",
    "                                     Shape: (n_samples, n_classes).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The gradient dZ.\n",
    "        \"\"\"\n",
    "        Z = self.cache\n",
    "        if Z is None:\n",
    "            raise ValueError(\"Cache is empty. Forward pass must be called before backward pass.\")\n",
    "\n",
    "        dZ = None\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            if dA is None: raise ValueError(\"dA must be provided for sigmoid backward pass.\")\n",
    "            s = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n",
    "            dZ = dA * s * (1 - s)\n",
    "        elif self.activation_function == \"relu\":\n",
    "            if dA is None: raise ValueError(\"dA must be provided for relu backward pass.\")\n",
    "            dZ = dA * (Z > 0).astype(dA.dtype) # Ensure boolean mask is converted to dA's type\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            if Y is None: \n",
    "                raise ValueError(\"Y (true labels) must be provided for softmax backward pass (when combined with CCE loss).\")\n",
    "            \n",
    "            if Z.ndim == 1: Z_reshaped = Z.reshape(1, -1)\n",
    "            else: Z_reshaped = Z\n",
    "            b = np.max(Z_reshaped, axis=1, keepdims=True)\n",
    "            exp_Z_shifted = np.exp(Z_reshaped - b)\n",
    "            s = exp_Z_shifted / np.sum(exp_Z_shifted, axis=1, keepdims=True)\n",
    "            if Z.ndim == 1: s = s.flatten()\n",
    "            \n",
    "            if s.shape != Y.shape:\n",
    "                raise ValueError(f\"Shape mismatch between softmax output s ({s.shape}) and labels Y ({Y.shape}). Ensure Y is one-hot encoded.\")\n",
    "            dZ = s - Y\n",
    "        elif self.activation_function == \"linear\":\n",
    "            if dA is None: raise ValueError(\"dA must be provided for linear backward pass.\")\n",
    "            dZ = dA.copy()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function for backward pass: {self.activation_function}\")\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for Activation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- Activation Layer Forward Pass Examples ---\")\n",
    "Z_sample_act = np.array([[-2, -1, 0, 1, 2], [0.5, -0.5, 1.5, -1.5, 0]], dtype=float)\n",
    "activation_sigmoid_ex = Activation(activation_function=\"sigmoid\")\n",
    "A_sigmoid_ex = activation_sigmoid_ex.forward(Z_sample_act.copy())\n",
    "activation_relu_ex = Activation(activation_function=\"relu\")\n",
    "A_relu_ex = activation_relu_ex.forward(Z_sample_act.copy())\n",
    "Z_softmax_sample_ex = np.array([[1.0, 2.0, 0.5], [-1.0, 3.0, 1.0], [0.0, 0.0, 0.0]])\n",
    "activation_softmax_ex = Activation(activation_function=\"softmax\")\n",
    "A_softmax_ex = activation_softmax_ex.forward(Z_softmax_sample_ex.copy())\n",
    "activation_linear_ex = Activation(activation_function=\"linear\")\n",
    "A_linear_ex = activation_linear_ex.forward(Z_sample_act.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n--- Activation Layer Backward Pass Examples ---\")\n",
    "dA_sample = np.random.randn(*Z_sample_act.shape) \n",
    "dZ_sigmoid = activation_sigmoid_ex.backward(dA=dA_sample)\n",
    "dZ_relu = activation_relu_ex.backward(dA=dA_sample)\n",
    "Y_sample_softmax = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]]) \n",
    "dZ_softmax = activation_softmax_ex.backward(Y=Y_sample_softmax)\n",
    "dZ_linear = activation_linear_ex.backward(dA=dA_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Class\n",
    "\n",
    "This class encapsulates the entire neural network model, composed of a sequence of Dense and Activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Represents a sequential neural network model.\n",
    "\n",
    "    Attributes:\n",
    "        units (list): List of integers defining the number of units in each layer,\n",
    "                      starting with the input dimension, followed by hidden layers, \n",
    "                      and ending with the output layer dimension.\n",
    "        activation_functions (list): List of strings specifying the activation function\n",
    "                                     for each layer transition (length should be len(units) - 1).\n",
    "        loss_function (str): The loss function to be used for training \n",
    "                             (e.g., 'cross_entropy', 'mse').\n",
    "        linear (list): List of Dense layer objects.\n",
    "        activation (list): List of Activation layer objects.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation_functions, loss_function):\n",
    "        \"\"\"Initializes the Model.\n",
    "\n",
    "        Args:\n",
    "            units (list): Number of units in each layer [input_dim, hidden1_dim, ..., output_dim].\n",
    "            activation_functions (list): Activation function for each layer transition.\n",
    "            loss_function (str): Loss function for the model.\n",
    "        \"\"\"\n",
    "        if len(units) - 1 != len(activation_functions):\n",
    "            raise ValueError(\"Number of activation functions must be one less than the number of unit layers.\")\n",
    "        \n",
    "        self.units = units\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.linear = []\n",
    "        self.activation = []\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Initializes all Dense and Activation layers in the model.\"\"\"\n",
    "        self.linear = []\n",
    "        self.activation = []\n",
    "        \n",
    "        # Create Dense layers\n",
    "        for i in range(len(self.units) - 1):\n",
    "            # seed=i ensures that each layer has a different initialization if seeds are used sequentially\n",
    "            dense_layer = Dense(self.units[i], self.units[i+1], seed=i)\n",
    "            self.linear.append(dense_layer)\n",
    "            \n",
    "        # Create Activation layers\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            current_loss_function_for_activation = None\n",
    "            if self.activation_functions[i] == 'softmax' and i == len(self.activation_functions) -1:\n",
    "                if self.loss_function == 'cross_entropy':\n",
    "                    current_loss_function_for_activation = self.loss_function \n",
    "            \n",
    "            activation_layer = Activation(self.activation_functions[i], loss_function=current_loss_function_for_activation)\n",
    "            self.activation.append(activation_layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Performs a full forward pass through all layers of the model.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Input data, shape (n_samples, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The final activated output of the model (AL).\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        for i in range(len(self.linear)):\n",
    "            Z = self.linear[i].forward(A)\n",
    "            A = self.activation[i].forward(Z)\n",
    "        return A # This is AL, the output of the last activation layer\n",
    "\n",
    "    def backward(self, AL, Y):\n",
    "        \"\"\"Performs a full backward pass through all layers of the model.\n",
    "\n",
    "        Args:\n",
    "            AL (np.ndarray): The output of the last activation layer (from forward pass).\n",
    "            Y (np.ndarray): True labels. Shape (n_samples, output_dim) for CCE (one-hot) \n",
    "                            or (n_samples, 1) for BCE/MSE.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Gradient with respect to the input X (dA0), not typically used further.\n",
    "        \"\"\"\n",
    "        L = len(self.linear) # Number of dense layers\n",
    "        dA_prev = None # Initialize dA_prev for the loop\n",
    "\n",
    "        # Initial gradient calculation (for the last layer)\n",
    "        last_activation_layer = self.activation[L-1]\n",
    "        \n",
    "        if last_activation_layer.activation_function == \"softmax\":\n",
    "            if self.loss_function != 'cross_entropy':\n",
    "                 print(\"Warning: Softmax output layer usually paired with cross_entropy loss for A-Y derivative.\")\n",
    "            dZ = last_activation_layer.backward(Y=Y) \n",
    "        elif last_activation_layer.activation_function == \"sigmoid\":\n",
    "            if self.loss_function != 'cross_entropy': \n",
    "                 print(\"Warning: Sigmoid output layer with cross_entropy typically means Binary Cross-Entropy.\")\n",
    "            epsilon = 1e-5 \n",
    "            if Y.shape != AL.shape:\n",
    "                if Y.ndim == 1: Y = Y.reshape(-1,1) \n",
    "                if Y.shape != AL.shape: \n",
    "                    raise ValueError(f\"Shape mismatch between Y ({Y.shape}) and AL ({AL.shape}) for sigmoid derivative.\")\n",
    "            dAL = - (np.divide(Y, AL + epsilon) - np.divide(1 - Y, 1 - AL + epsilon))\n",
    "            dZ = last_activation_layer.backward(dA=dAL)\n",
    "        elif last_activation_layer.activation_function == \"linear\":\n",
    "            if self.loss_function != 'mse':\n",
    "                print(\"Warning: Linear output layer with non-MSE loss might require different dAL.\")\n",
    "            if Y.shape != AL.shape:\n",
    "                if Y.ndim == 1: Y = Y.reshape(-1,1)\n",
    "                if Y.shape != AL.shape: \n",
    "                    raise ValueError(f\"Shape mismatch between Y ({Y.shape}) and AL ({AL.shape}) for linear derivative.\")\n",
    "            dAL = AL - Y \n",
    "            dZ = last_activation_layer.backward(dA=dAL)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function for the last layer's backward pass: {last_activation_layer.activation_function}\")\n",
    "\n",
    "        dA_prev = self.linear[L-1].backward(dZ)\n",
    "\n",
    "        for l in range(L-2, -1, -1):\n",
    "            dZ = self.activation[l].backward(dA=dA_prev)\n",
    "            dA_prev = self.linear[l].backward(dZ)\n",
    "            \n",
    "        return dA_prev \n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"Updates parameters for all Dense layers in the model.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): The learning rate to apply.\n",
    "        \"\"\"\n",
    "        if not self.linear: # Check if there are any layers to update\n",
    "            print(\"Warning: Model has no layers to update.\")\n",
    "            return\n",
    "            \n",
    "        for l in range(len(self.linear)):\n",
    "            # The Dense layer's update method uses its own self.dW and self.db,\n",
    "            # which should have been populated by the Model's backward pass.\n",
    "            self.linear[l].update(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization and Forward Pass Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model Class Example ---\")\n",
    "units_example = [3, 5, 1] # Input=3, Hidden=5, Output=1\n",
    "activations_example = [\"relu\", \"sigmoid\"]\n",
    "loss_example = \"cross_entropy\" # For BCE with sigmoid output\n",
    "\n",
    "model_ex = Model(units=units_example, activation_functions=activations_example, loss_function=loss_example)\n",
    "print(f\"Model initialized with {len(model_ex.linear)} Dense layers and {len(model_ex.activation)} Activation layers.\")\n",
    "\n",
    "X_model_sample = np.array([[1.0, -2.0, 0.5], [-0.5, 1.5, -1.0]])\n",
    "AL_output = model_ex.forward(X_model_sample)\n",
    "print(f\"\\nModel forward pass output AL (shape {AL_output.shape}):\\n{AL_output}\")\n",
    "\n",
    "units_softmax = [4, 6, 3] \n",
    "activations_softmax = [\"relu\", \"softmax\"]\n",
    "model_softmax_ex = Model(units_softmax, activations_softmax, \"cross_entropy\")\n",
    "X_softmax_input = np.random.randn(5, 4) \n",
    "AL_softmax_output = model_softmax_ex.forward(X_softmax_input)\n",
    "# print(f\"Softmax Model AL output (shape {AL_softmax_output.shape}):\\n{AL_softmax_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Backward Pass Example (Sigmoid Output) ---\")\n",
    "Y_sample_sigmoid = np.array([[1], [0]], dtype=float) \n",
    "dA0_sigmoid = model_ex.backward(AL_output, Y_sample_sigmoid)\n",
    "# print(f\"\\ndA0 from backward pass (sigmoid model, shape {dA0_sigmoid.shape}):\\n{dA0_sigmoid}\")\n",
    "\n",
    "print(\"\\n--- Model Backward Pass Example (Softmax Output) ---\")\n",
    "Y_sample_softmax = np.array([[0,1,0], [1,0,0], [0,0,1], [0,1,0], [1,0,0]], dtype=float) \n",
    "dA0_softmax = model_softmax_ex.backward(AL_softmax_output, Y_sample_softmax)\n",
    "# print(f\"\\ndA0 from backward pass (softmax model, shape {dA0_softmax.shape}):\\n{dA0_softmax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Method Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Update Example (using Sigmoid Output Model) ---\")\n",
    "learning_rate_model = 0.01\n",
    "print(f\"Learning rate: {learning_rate_model}\")\n",
    "\n",
    "# Store weights of the first Dense layer before update\n",
    "W0_before_update = model_ex.linear[0].parameters[\"W\"].copy()\n",
    "b0_before_update = model_ex.linear[0].parameters[\"b\"].copy()\n",
    "\n",
    "print(f\"Layer 0 W before update (first 2 rows):\\n{W0_before_update[:2]}\")\n",
    "print(f\"Layer 0 b before update:\\n{b0_before_update}\")\n",
    "\n",
    "model_ex.update(learning_rate_model)\n",
    "\n",
    "W0_after_update = model_ex.linear[0].parameters[\"W\"]\n",
    "b0_after_update = model_ex.linear[0].parameters[\"b\"]\n",
    "\n",
    "print(f\"\\nLayer 0 W after update (first 2 rows):\\n{W0_after_update[:2]}\")\n",
    "print(f\"Layer 0 b after update:\\n{b0_after_update}\")\n",
    "\n",
    "# Verify parameters changed\n",
    "if not np.array_equal(W0_before_update, W0_after_update):\n",
    "    print(\"Layer 0 W successfully updated.\")\n",
    "else:\n",
    "    print(\"Warning: Layer 0 W did not change after update!\")\n",
    "\n",
    "if not np.array_equal(b0_before_update, b0_after_update):\n",
    "    print(\"Layer 0 b successfully updated.\")\n",
    "else:\n",
    "    print(\"Warning: Layer 0 b did not change after update!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

[end of lab4_neural_network/Lab4.ipynb]

[end of lab4_neural_network/Lab4.ipynb]

[end of lab4_neural_network/Lab4.ipynb]

[end of lab4_neural_network/Lab4.ipynb]
