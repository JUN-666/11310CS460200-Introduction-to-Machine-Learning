{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math # For math.floor in random_mini_batches, if used later\n",
    "import matplotlib.pyplot as plt # For plot_losses method\n",
    "from tqdm import tqdm # For train method progress bar\n",
    "import pandas as pd # For data handling if needed in other parts\n",
    "from sklearn.model_selection import train_test_split # Added for data splitting\n",
    "\n",
    "# Import from .py files in the same directory (dummy files created earlier)\n",
    "from Dense import Dense\n",
    "from Activation import Activation\n",
    "from Loss import compute_CCE_loss, compute_MSE_loss # Assuming these are in Loss.py\n",
    "from Flatten import Flatten\n",
    "# RNN class is defined in this notebook below\n",
    "\n",
    "outputs = {} # Initialize the outputs dictionary globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, input_size, rnn_units, seed=1):\n",
    "        self.input_size = input_size\n",
    "        self.rnn_units = rnn_units\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "        # Initialize other necessary attributes for later methods if known\n",
    "        self.xs = [] # Will store inputs x_t for each timestep\n",
    "        self.hs = [] # Will store hidden states h_t for each timestep\n",
    "        self.h = None # Represents the current/last hidden state\n",
    "        self.dL_dWx = None\n",
    "        self.dL_dWh = None\n",
    "        self.dL_dbh = None\n",
    "\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases for the RNN layer.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Multiply by 0.01 is for testing reason.\n",
    "        # Wx: Input to hidden weights , with shape of (rnn_units, input_size).\n",
    "        self.Wx = np.random.randn(self.rnn_units, self.input_size) * 0.01\n",
    "        # Wh: Hidden to hidden weights, with shape of (rnn_units, rnn_units).\n",
    "        self.Wh = np.random.randn(self.rnn_units, self.rnn_units) * 0.01\n",
    "        # bh: Hidden bias, with shape of (rnn_units, 1).\n",
    "        self.bh = np.zeros((self.rnn_units, 1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the RNN layer.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Input data of shape (batch_size, timesteps, input_size).\n",
    "\n",
    "        Returns:\n",
    "        ndarray: Output of the RNN layer (last hidden state).\n",
    "        \"\"\"\n",
    "        batch_size, timesteps, _ = X.shape\n",
    "        ### START CODE HERE ###\n",
    "        # self.h: Initialize hidden state, with shape of (batch_size, self.rnn_units).\n",
    "        self.h = np.zeros((batch_size, self.rnn_units))\n",
    "        # self.hs: Store hidden states for backward pass, list of self.h.\n",
    "        # This list will store h_0, h_1, ..., h_{timesteps-1} (states after processing input)\n",
    "        self.hs = [] \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        self.xs = []  # Store inputs for backward pass\n",
    "\n",
    "        for t in range(timesteps):\n",
    "            ### START CODE HERE ###\n",
    "            # x_t: Get input at time step t for all data in X. Shape (batch_size, input_size)\n",
    "            x_t = X[:, t, :]              \n",
    "            self.xs.append(x_t) # Store x_t\n",
    "\n",
    "            # self.h: Update hidden state according to the formula of h_t.\n",
    "            # self.h on the right side is h_{t-1} from previous step (or initial zeros)\n",
    "            self.h = np.tanh(np.dot(x_t, self.Wx.T) + np.dot(self.h, self.Wh.T) + self.bh.T)\n",
    "            \n",
    "            # self.hs.append(self.h) : Store the new self.h (which is h_t)\n",
    "            self.hs.append(np.copy(self.h)) # Store a copy\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "        return self.h # Return the last hidden state\n",
    "\n",
    "    def backward(self, dH):\n",
    "        \"\"\"\n",
    "        Perform the backward pass through the RNN layer.\n",
    "\n",
    "        Parameters:\n",
    "        dH (ndarray): Gradient of the loss with respect to the final hidden state output. Shape (batch_size, rnn_units).\n",
    "        # clip_value (float): Value to clip the gradients to prevent exploding gradients. (Not used now)\n",
    "\n",
    "        Returns:\n",
    "        ndarray: Gradient of the loss with respect to the initial hidden state (or input if it were an input).\n",
    "        \"\"\"\n",
    "        # If dH comes from a source that averages over batch, then batch_size here might be 1.\n",
    "        # However, typically dH is (batch_size, rnn_units).\n",
    "        # Let's assume batch_size is determinable from dH or self.xs if available.\n",
    "        # If self.xs is populated, self.xs[0].shape[0] is batch_size.\n",
    "        batch_size = self.xs[0].shape[0] if self.xs else dH.shape[0]\n",
    "\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Initialize gradients with zeros, same shapes as parameters\n",
    "        self.dL_dWx = np.zeros_like(self.Wx)      \n",
    "        self.dL_dWh = np.zeros_like(self.Wh)      \n",
    "        self.dL_dbh = np.zeros_like(self.bh)      \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initialize dL_dh_next (gradient of loss w.r.t. hidden state h_t from next step t+1 or layer above)\n",
    "        # For the last time step, this is the dH passed to the function.\n",
    "        dL_dh_next = dH \n",
    "\n",
    "        # Loop backwards through time\n",
    "        # len(self.hs) is 'timesteps'. Loop t from timesteps-1 down to 0.\n",
    "        for t in reversed(range(len(self.hs))):\n",
    "            # Get x_t (input at time t), h_t (hidden state at time t)\n",
    "            x_t = self.xs[t]  \n",
    "            h_t = self.hs[t]  # h_t = tanh(Wx*x_t + Wh*h_{t-1} + bh)\n",
    "            \n",
    "            # Get h_prev_t (hidden state at time t-1)\n",
    "            # If t=0, h_prev_t is the initial hidden state (all zeros)\n",
    "            h_prev_t = self.hs[t-1] if t > 0 else np.zeros((batch_size, self.rnn_units))\n",
    "            \n",
    "            ### START CODE HERE ###\n",
    "            # Compute dL_dh_raw: gradient of loss w.r.t. the raw activation (input to tanh) at time t\n",
    "            # dL_dh_raw = dL/dh_t * dh_t/dh_raw_t = dL_dh_next * (1 - h_t^2)\n",
    "            dL_dh_raw = (1 - h_t**2) * dL_dh_next     # Derivative of tanh is (1 - tanh^2)\n",
    "\n",
    "            # Compute gradients for Wx, Wh, bh at time t and accumulate\n",
    "            # dL/dWx = sum over t (dL/dh_raw_t * dh_raw_t/dWx) = sum over t (dL_dh_raw_t * x_t.T)\n",
    "            # Transpose dL_dh_raw to (rnn_units, batch_size) for dot product with x_t (batch_size, input_size)\n",
    "            self.dL_dWx += np.dot(dL_dh_raw.T, x_t)  \n",
    "            \n",
    "            # dL/dWh = sum over t (dL/dh_raw_t * dh_raw_t/dWh) = sum over t (dL_dh_raw_t * h_{t-1}.T)\n",
    "            # Transpose dL_dh_raw for dot product with h_prev_t\n",
    "            self.dL_dWh += np.dot(dL_dh_raw.T, h_prev_t) \n",
    "            \n",
    "            # dL/dbh = sum over t (dL/dh_raw_t * dh_raw_t/dbh) = sum over t (dL_dh_raw_t * 1)\n",
    "            # Sum dL_dh_raw over batch dimension, keep dims, then transpose to match bh shape (rnn_units, 1)\n",
    "            self.dL_dbh += np.sum(dL_dh_raw, axis=0, keepdims=True).T \n",
    "\n",
    "            # Compute dL_dh_next for the *previous* time step (t-1)\n",
    "            # This is dL/dh_{t-1} = dL/dh_raw_t * dh_raw_t/dh_{t-1} = dL_dh_raw * Wh\n",
    "            # dL_dh_raw is (batch_size, rnn_units), Wh is (rnn_units, rnn_units)\n",
    "            dL_dh_next = np.dot(dL_dh_raw, self.Wh) # This will be used in the next iteration (for t-1)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Average gradients over the batch.\n",
    "        self.dL_dWx /= batch_size\n",
    "        self.dL_dWh /= batch_size\n",
    "        self.dL_dbh /= batch_size\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Gradient clipping part (optional, commented out for now as per instructions for testing)\n",
    "        # clip_value = 2.0 \n",
    "        # np.clip(self.dL_dWx, -clip_value, clip_value, out=self.dL_dWx)\n",
    "        # np.clip(self.dL_dWh, -clip_value, clip_value, out=self.dL_dWh)\n",
    "        # np.clip(self.dL_dbh, -clip_value, clip_value, out=self.dL_dbh)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dL_dh_next # This is dL/dh_{-1} (gradient w.r.t. initial hidden state)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and biases using the computed gradients.\n",
    "\n",
    "        Parameters:\n",
    "        learning_rate (float): Learning rate for weight updates.\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # Update each weight parameter using its gradient and the learning rate.\n",
    "        self.Wx -= learning_rate * self.dL_dWx\n",
    "        self.Wh -= learning_rate * self.dL_dWh\n",
    "        self.bh -= learning_rate * self.dL_dbh\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self): # Corrected to __init__\n",
    "        self.layers = []\n",
    "        # For Lab 6, train_losses and val_losses are part of the Model for the train method\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\" Sequentially add a layer into the model.\n",
    "\n",
    "        Parameters:\n",
    "        layer: Different layers class.\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        self.layers.append(layer)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\" Implement the forward propagation for the model.\n",
    "        \n",
    "        Arguments:\n",
    "        X -- input data\n",
    "        \n",
    "        Returns:\n",
    "        A -- output of the last layer\n",
    "        \"\"\"\n",
    "        # X will be updated as it passes through layers\n",
    "        for layer in self.layers:\n",
    "            if layer.__class__.__name__ == 'RNN': # Check if it's an RNN layer\n",
    "                if len(X.shape) < 3: # If X (output of prev layer) is 2D (e.g., from Dense or a flattened RNN output)\n",
    "                    ### START CODE HERE ###\n",
    "                    # Reshape X to be (batch_size, timesteps=1, features=X.shape[1])\n",
    "                    # This treats the entire previous output as a single time step input to this RNN.\n",
    "                    X = X.reshape(X.shape[0], 1, X.shape[1]) \n",
    "                    ### END CODE HERE ###\n",
    "                X = layer.forward(X) # Call RNN's forward pass\n",
    "            else: # For non-RNN layers (Dense, Activation, Flatten)\n",
    "                X = layer.forward(X) # Call their forward pass\n",
    "        return X # Return the final output\n",
    "\n",
    "    def backward(self, dA, Y=None):\n",
    "        \"\"\" Implement the backward propagation for the model.\n",
    "        \n",
    "        Arguments:\n",
    "        dA -- gradient of the loss with respect to the output of the model (AL).\n",
    "              For CCE with Softmax, this is typically AL - Y.\n",
    "        Y -- true labels, needed if the final activation is softmax and CCE is used (shape depends on loss & activation combo)\n",
    "        \n",
    "        Returns:\n",
    "        dA_prev -- gradient with respect to the input of the first layer\n",
    "        \"\"\"\n",
    "        # Iterate backwards through the layers\n",
    "        # The initial dA is dL/dAL (gradient of loss w.r.t. output of the last layer)\n",
    "        # or it could be dL/dZ if Softmax+CCE derivative (AL-Y) is passed directly.\n",
    "        # The prompt's logic implies dA is dL/dAL and Softmax layer's backward handles Y to get dL/dZ.\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Activation) and layer.activation_function == \"softmax\":\n",
    "                # Special case for Softmax activation: its backward method might use Y directly\n",
    "                # to compute dZ = AL - Y (if dA is effectively ignored or used as AL by the method)\n",
    "                # or it computes dL/dZ = (dL/dAL) * (dAL/dZ) where dL/dAL is the incoming dA.\n",
    "                # Given the prompt's specific call signature `layer.backward(Y=Y)`, we follow that.\n",
    "                ### START CODE HERE ###\n",
    "                dA = layer.backward(Y=Y) # This specific call is for Softmax activation\n",
    "                ### END CODE HERE ###\n",
    "            else:\n",
    "                # For other layers (Dense, RNN, Flatten, other Activations),\n",
    "                # their backward method takes the gradient dA from the next layer \n",
    "                # (which is dL/d(output_of_current_layer))\n",
    "                # and computes the gradient w.r.t. their own input (dL/d(input_of_current_layer)).\n",
    "                ### START CODE HERE ###\n",
    "                dA = layer.backward(dA) \n",
    "                ### END CODE HERE ###\n",
    "        return dA # Final dA is the gradient w.r.t. the input of the first layer\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # Update parameters for layers that have an 'update' method (e.g., Dense, RNN)\n",
    "        ### START CODE HERE ###\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update'):\n",
    "                layer.update(learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=10, learning_rate=0.001, batch_size=32, loss_function='mse'):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        m = X_train.shape[0]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss_epoch = 0\n",
    "\n",
    "            permutation = np.random.permutation(m)\n",
    "            shuffled_X = X_train[permutation]\n",
    "            shuffled_Y = y_train[permutation]\n",
    "\n",
    "            ### START CODE HERE ###\n",
    "            num_complete_minibatches = math.floor(m / batch_size)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            with tqdm(total=m, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"sample\") as pbar:\n",
    "                for k in range(num_complete_minibatches):\n",
    "                    ### START CODE HERE ###\n",
    "                    X_batch = shuffled_X[k * batch_size : (k + 1) * batch_size]\n",
    "                    y_batch = shuffled_Y[k * batch_size : (k + 1) * batch_size]\n",
    "                    ### END CODE HERE ###\n",
    "\n",
    "                    ### START CODE HERE ###\n",
    "                    # 1. Forward to get the prediction.\n",
    "                    y_pred = self.forward(X_batch)\n",
    "                    \n",
    "                    # 2. Calculate the loss.\n",
    "                    current_batch_loss = 0\n",
    "                    if loss_function == 'cce':\n",
    "                        current_batch_loss = compute_CCE_loss(y_batch, y_pred)\n",
    "                    elif loss_function == 'mse':\n",
    "                        current_batch_loss = compute_MSE_loss(y_batch, y_pred)\n",
    "                    else:\n",
    "                        raise ValueError(\"Unsupported loss function: \" + loss_function)\n",
    "                    total_loss_epoch += current_batch_loss * X_batch.shape[0]\n",
    "\n",
    "                    # 3. Calculate dA.\n",
    "                    dA = y_pred - y_batch # This is AL - Y for CCE+Softmax or MSE with linear output\n",
    "                    \n",
    "                    # 4. backward with the calculated dA.\n",
    "                    self.backward(dA, y_batch) # Pass y_batch for softmax layer if needed by its backward\n",
    "                    \n",
    "                    # 5. update the parameters.\n",
    "                    self.update(learning_rate)\n",
    "                    ### END CODE HERE ###\n",
    "                    pbar.update(X_batch.shape[0])\n",
    "                    if (k + 1) % 5 == 0:\n",
    "                        pbar.set_postfix(loss=(total_loss_epoch / ((k + 1) * batch_size)))\n",
    "\n",
    "                # Handle the remaining examples that do not fit into a full batch\n",
    "                if m % batch_size != 0:\n",
    "                    ### START CODE HERE ###\n",
    "                    X_batch_remainder = shuffled_X[num_complete_minibatches * batch_size:]\n",
    "                    y_batch_remainder = shuffled_Y[num_complete_minibatches * batch_size:]\n",
    "                    ### END CODE HERE ###\n",
    "\n",
    "                    if X_batch_remainder.shape[0] > 0:\n",
    "                        ### START CODE HERE ###\n",
    "                        y_pred_remainder = self.forward(X_batch_remainder)\n",
    "                        current_batch_loss_remainder = 0\n",
    "                        if loss_function == 'cce':\n",
    "                            current_batch_loss_remainder = compute_CCE_loss(y_batch_remainder, y_pred_remainder)\n",
    "                        elif loss_function == 'mse':\n",
    "                            current_batch_loss_remainder = compute_MSE_loss(y_batch_remainder, y_pred_remainder)\n",
    "                        else:\n",
    "                            raise ValueError(\"Unsupported loss function\")\n",
    "                        total_loss_epoch += current_batch_loss_remainder * X_batch_remainder.shape[0]\n",
    "                        \n",
    "                        dA_remainder = y_pred_remainder - y_batch_remainder\n",
    "                        self.backward(dA_remainder, y_batch_remainder)\n",
    "                        self.update(learning_rate)\n",
    "                        ### END CODE HERE ###\n",
    "                        pbar.update(X_batch_remainder.shape[0])\n",
    "            \n",
    "            ### START CODE HERE ###\n",
    "            avg_train_loss = total_loss_epoch / m\n",
    "            ### END CODE HERE ###\n",
    "            self.train_losses.append(avg_train_loss)\n",
    "            # Use a more robust f-string for printing to avoid potential TypeError with older % formatting\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.6f}')\n",
    "\n",
    "            # Validation part\n",
    "            ### START CODE HERE ###\n",
    "            y_pred_val = self.forward(X_val)\n",
    "            val_loss = 0\n",
    "            if loss_function == 'cce':\n",
    "                val_loss = compute_CCE_loss(y_val, y_pred_val)\n",
    "            elif loss_function == 'mse':\n",
    "                val_loss = compute_MSE_loss(y_val, y_pred_val)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported loss function\")\n",
    "            ### END CODE HERE ###\n",
    "            self.val_losses.append(val_loss)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.6f}')\n",
    "\n",
    "    def predict(self, X): # predict is a simple forward pass\n",
    "        return self.forward(X)\n",
    "\n",
    "    def plot_losses(self):\n",
    "        # Plotting training and validation losses\n",
    "        ### START CODE HERE ###\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Sinusoidal wave dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sine_wave_data(num_samples, num_timesteps, freq_range, amp_range):\n",
    "    \"\"\" Generate sine wave data with varying frequencies, amplitudes, and random phase shifts.\n",
    "    The last value in each sequence is used as the target prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - num_samples: Number of samples to generate.\n",
    "    - num_timesteps: Number of timesteps for each sample.\n",
    "    - freq_range: Tuple of floor and ceiling of frequency range.\n",
    "    - amp_range: Tuple of floor and ceiling of amplitude range.\n",
    "\n",
    "    Returns:\n",
    "    - X: Generated sine wave data of shape (num_samples, num_timesteps - 1).\n",
    "    - y: Target values of shape (num_samples,).\n",
    "    \"\"\"\n",
    "    X = np.zeros((num_samples, num_timesteps - 1))\n",
    "    y = np.zeros(num_samples)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        ### START CODE HERE ###\n",
    "        # Choose the frequency, amplitude and shift phase value.\n",
    "        freq = np.random.uniform(freq_range[0], freq_range[1])\n",
    "        amp = np.random.uniform(amp_range[0], amp_range[1])\n",
    "        phase_shift = np.random.uniform(0, 2 * np.pi)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        t = np.linspace(0, 2 * np.pi, num_timesteps)\n",
    "        sine_wave = amp * np.sin(freq * t + phase_shift)\n",
    "        X[i] = sine_wave[:-1]  # All but the last value\n",
    "        y[i] = sine_wave[-1]   # The last value\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "num_samples = 1600 \n",
    "num_timesteps = 100 \n",
    "freq_range = (0.5, 10.0) \n",
    "amp_range = (0.5, 10.0)\n",
    "### END CODE HERE ###\n",
    "\n",
    "X, y = generate_sine_wave_data(num_samples, num_timesteps, freq_range, amp_range)\n",
    "\n",
    "### START CODE HERE ###\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "### END CODE HERE ###\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_train[0], label='Input Sequence (X_train[0])') \n",
    "plt.plot(num_timesteps -1, y_train[0], 'ro', label=f'Target Value (y_train[0]) at t={num_timesteps-1}') \n",
    "plt.xlabel(f\"Time step (X has {num_timesteps-1} steps)\") \n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Example Sinusoidal Wave from Training Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct the model with Dense layer only"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Reshape y_train and y_val for the dense model (if they are not already column vectors)\n",
    "input_features = X_train.shape[1] # Number of features for the dense model is num_timesteps - 1\n",
    "y_train_dense = y_train.reshape(-1, 1)\n",
    "y_val_dense = y_val.reshape(-1, 1)\n",
    "\n",
    "model_dense = Model()\n",
    "model_dense.add(Dense(input_dim=input_features, output_dim=64)) # Using updated Dense __init__\n",
    "model_dense.add(Activation(\"relu\", None))\n",
    "model_dense.add(Dense(input_dim=64, output_dim=1))\n",
    "model_dense.add(Activation(\"linear\", None))\n",
    "\n",
    "model_dense.train(X_train, y_train_dense, X_val, y_val_dense, epochs=20, learning_rate=0.001, batch_size=8, loss_function='mse')\n",
    "### END CODE HERE ###\n",
    "model_dense.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Read the X_test.csv file into a DataFrame\n",
    "# Change the path if needed. Assumes 'X_test.csv' is in a subdir 'Sinewave' relative to the notebook.\n",
    "# The user needs to download this file from Kaggle.\n",
    "try:\n",
    "    X_test_df = pd.read_csv('Sinewave/X_test.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Sinewave/X_test.csv not found. Please download it from Kaggle and place it in the correct path.\")\n",
    "    # Create a dummy X_test_df for subsequent code to run without error, user should replace this.\n",
    "    # num_timesteps-1 features are expected by the dense model.\n",
    "    # X_train.shape[1] holds this value (num_timesteps - 1)\n",
    "    num_features = X_train.shape[1] # Should be num_timesteps - 1\n",
    "    X_test_df = pd.DataFrame(np.random.rand(10, num_features)) # Dummy with 10 samples\n",
    "    # X_test_df.insert(0, 'Id', range(10)) # Dummy Id column, Kaggle IDs are 1-based. Let's make it 1-based for consistency.\n",
    "    X_test_df.insert(0, 'Id', range(1, 11)) # Dummy Id column, 1-based\n",
    "    print(f\"Created a DUMMY X_test_df with shape {X_test_df.shape} for placeholder.\")\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Do not modify this part to get the correct output format!! (as per prompt, though we'll integrate)\n",
    "# Drop the 'Id' column if it exists for feature processing, but save original IDs for submission\n",
    "if 'Id' in X_test_df.columns:\n",
    "    X_test_kaggle_ids = X_test_df['Id'] \n",
    "    X_test_df_features = X_test_df.drop(columns=['Id'])\n",
    "else:\n",
    "    # If no 'Id' column, generate 1-based IDs matching the length of the test data\n",
    "    X_test_kaggle_ids = range(1, len(X_test_df) + 1) \n",
    "    X_test_df_features = X_test_df.copy()\n",
    "\n",
    "# Convert the DataFrame to a numpy array\n",
    "X_test_sine = X_test_df_features.values \n",
    "# Ensure X_test_sine has the same number of features as X_train for the dense model\n",
    "if X_test_sine.shape[1] != X_train.shape[1]:\n",
    "    print(f\"Warning: X_test_sine feature count ({X_test_sine.shape[1]}) differs from X_train ({X_train.shape[1]}). Ensure data is correct, or adapt dummy data creation.\")\n",
    "    # This might happen if dummy X_test_df creation (if file not found) used a different feature count\n",
    "    # than the actual X_train. For robustness, one might re-calculate num_features here if X_train is available.\n",
    "    # For now, we assume X_train is available if dummy was created.\n",
    "\n",
    "y_pred_dense = model_dense.predict(X_test_sine)\n",
    "\n",
    "# Convert the list of predictions to a numpy array and ensure it's flat\n",
    "y_pred_dense_flat = np.array(y_pred_dense).flatten()\n",
    "\n",
    "y_pred_df = pd.DataFrame({\n",
    "    'Id': X_test_kaggle_ids, \n",
    "    'answer': y_pred_dense_flat\n",
    "})\n",
    "output_csv_name = 'y_pred_dense_sinewave.csv'\n",
    "y_pred_df.to_csv(output_csv_name, index=False)\n",
    "\n",
    "print(f\"Prediction data for Dense model saved to {output_csv_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct the model with RNN layer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Change the structure and parameters to train your own model (Can add RNN layer here)\n",
    "\n",
    "# Reshape X_train and X_val to fit the RNN layer input shape.\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_val_rnn = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "y_train_rnn = y_train.reshape(-1, 1)\n",
    "y_val_rnn = y_val.reshape(-1, 1)\n",
    "\n",
    "input_size = 1       \n",
    "rnn_units = 32       \n",
    "output_size = 1      \n",
    "\n",
    "model_rnn = Model()\n",
    "model_rnn.add(RNN(input_size=input_size, rnn_units=rnn_units)) \n",
    "model_rnn.add(Dense(input_dim=rnn_units, output_dim=output_size)) # Corrected Dense call\n",
    "model_rnn.add(Activation(\"linear\", None)) \n",
    "\n",
    "model_rnn.train(X_train_rnn, y_train_rnn, X_val_rnn, y_val_rnn, epochs=20, learning_rate=0.001, batch_size=16, loss_function='mse')\n",
    "### END CODE HERE ###\n",
    "\n",
    "model_rnn.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict testing data & Save the answer (For RNN model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember to submit your prediction to Kaggle!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Read the X_test.csv file into a DataFrame\n",
    "# Change the path if needed. Assumes 'X_test.csv' is in a subdir 'Sinewave' relative to the notebook.\n",
    "# The user needs to download this file from Kaggle.\n",
    "try:\n",
    "    X_test_df_rnn = pd.read_csv('Sinewave/X_test.csv') \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Sinewave/X_test.csv not found. Please download it from Kaggle and place it in the correct path.\")\n",
    "    num_features = X_train.shape[1] # Should be num_timesteps - 1 (from sine wave data generation)\n",
    "    X_test_df_rnn = pd.DataFrame(np.random.rand(10, num_features)) \n",
    "    X_test_df_rnn.insert(0, 'Id', range(1, 11)) \n",
    "    print(f\"Created a DUMMY X_test_df_rnn with shape {X_test_df_rnn.shape} for placeholder.\")\n",
    "### END CODE HERE ###\n",
    "\n",
    "if 'Id' in X_test_df_rnn.columns:\n",
    "    X_test_kaggle_ids_rnn = X_test_df_rnn['Id'] \n",
    "    X_test_df_features_rnn = X_test_df_rnn.drop(columns=['Id'])\n",
    "else:\n",
    "    X_test_df_features_rnn = X_test_df_rnn.copy() \n",
    "    X_test_kaggle_ids_rnn = range(1, len(X_test_df_features_rnn) + 1)\n",
    "\n",
    "X_test_sine_for_rnn = X_test_df_features_rnn.values \n",
    "\n",
    "# Reshape for RNN model\n",
    "X_test_input_rnn = X_test_sine_for_rnn.reshape(X_test_sine_for_rnn.shape[0], X_test_sine_for_rnn.shape[1], 1) \n",
    "\n",
    "y_pred_rnn_final = model_rnn.predict(X_test_input_rnn)\n",
    "\n",
    "y_pred_rnn_flat = np.array(y_pred_rnn_final).flatten()\n",
    "\n",
    "y_pred_df_final = pd.DataFrame({\n",
    "    'Id': X_test_kaggle_ids_rnn, \n",
    "    'answer': y_pred_rnn_flat\n",
    "})\n",
    "output_csv_name_rnn = 'y_pred_basic.csv' \n",
    "y_pred_df_final.to_csv(output_csv_name_rnn, index=False)\n",
    "\n",
    "print(f\"Prediction data for RNN model saved to {output_csv_name_rnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Advance part (35%)\n",
    "### Accelerometer and Gyroscope dataset\n",
    "⚠⚠ You need to download the training & testing data from Kaggle. ⚠⚠ \n",
    "(Put it into the directory name Activity data, or your need to change the path in template.)\n",
    "This dataset comprises data collected from accelerometer and gyroscope sensors from a smartphone carried by individuals while performing six different activities: WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, and LAYING.\n",
    "The goal of this task is to build a model that can accurately classify these activities based on the sensor readings.\n",
    "The data is preprocessed into sequences of a fixed length, making it suitable for RNNs. Each sequence represents a window of sensor readings over time.\n",
    "\n",
    "**Dataset Details**:\n",
    "- **X_train.npy**: Training data features. Shape: (num_samples, timesteps, num_features). `num_features` is 6 (3 accelerometer axes + 3 gyroscope axes).\n",
    "- **y_train.npy**: Training data labels, one-hot encoded. Shape: (num_samples, num_classes=6).\n",
    "- **X_test.npy**: Testing data features. Shape: (num_test_samples, timesteps, num_features).\n",
    "\n",
    "### 1. Load training & testing data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Load X_train, y_train and X_test data\n",
    "# Change the path if needed\n",
    "try:\n",
    "    y_train_activity_raw = np.load('Activity data/y_train.npy')\n",
    "    X_train_activity_raw = np.load('Activity data/X_train.npy')\n",
    "    X_test_activity = np.load('Activity data/X_test.npy')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please download data from Kaggle and place it in 'Activity data/' directory.\")\n",
    "    print(\"Creating DUMMY data for Activity dataset to allow notebook to run.\")\n",
    "    # X_train: (samples, timesteps, features=6)\n",
    "    # y_train: (samples, classes=6) - one-hot encoded\n",
    "    # X_test: (samples, timesteps, features=6)\n",
    "    X_train_activity_raw = np.random.rand(100, 50, 6) # 100 samples, 50 timesteps, 6 features\n",
    "    y_train_activity_raw = np.zeros((100, 6)).astype(float) # Dummy one-hot for 6 classes\n",
    "    for i_dummy in range(len(y_train_activity_raw)):\n",
    "        y_train_activity_raw[i_dummy, np.random.randint(0,6)] = 1.0 # Assign one hot\n",
    "    X_test_activity = np.random.rand(20, 50, 6) # 20 test samples\n",
    "\n",
    "print(f\"X_train_activity_raw shape: {X_train_activity_raw.shape}\")\n",
    "print(f\"y_train_activity_raw shape: {y_train_activity_raw.shape}\")\n",
    "print(f\"X_test_activity shape: {X_test_activity.shape}\")\n",
    "### END CODE HERE ###\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Define the validation ratio\n",
    "validation_ratio = 0.15 \n",
    "\n",
    "# Split the X_train, y_train data into train & validation set.\n",
    "# y_train_activity_raw is already one-hot encoded.\n",
    "X_train_adv, X_val_adv, y_train_adv, y_val_adv = train_test_split(\n",
    "    X_train_activity_raw, \n",
    "    y_train_activity_raw, \n",
    "    test_size=validation_ratio, \n",
    "    random_state=42, \n",
    "    stratify=y_train_activity_raw if y_train_activity_raw.ndim > 1 and y_train_activity_raw.shape[1] > 1 else None\n",
    ")\n",
    "\n",
    "print(f\"X_train_adv shape: {X_train_adv.shape}, y_train_adv shape: {y_train_adv.shape}\")\n",
    "print(f\"X_val_adv shape: {X_val_adv.shape}, y_val_adv shape: {y_val_adv.shape}\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Visualize the training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(X, y, sample_index):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(X[sample_index, :, 0], label='Acc X')\n",
    "    plt.plot(X[sample_index, :, 1], label='Acc Y')\n",
    "    plt.plot(X[sample_index, :, 2], label='Acc Z')\n",
    "    plt.plot(X[sample_index, :, 3], label='Gyro X')\n",
    "    plt.plot(X[sample_index, :, 4], label='Gyro Y')\n",
    "    plt.plot(X[sample_index, :, 5], label='Gyro Z')\n",
    "    activity_class = np.argmax(y[sample_index])\n",
    "    plt.title(f'Sample {sample_index} - Activity Class: {activity_class}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Sensor Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Plot a few samples\n",
    "if 'X_train_adv' in locals() and 'y_train_adv' in locals() and X_train_adv.shape[0] > 0 :\n",
    "    num_samples_to_plot = min(3, X_train_adv.shape[0]) \n",
    "    for i in range(num_samples_to_plot): \n",
    "        plot_sample(X_train_adv, y_train_adv, i)\n",
    "else:\n",
    "    print(\"X_train_adv or y_train_adv not defined or empty. Skipping visualization of Activity data.\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Construct and train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Ensure X_train_adv, y_train_adv, X_val_adv, y_val_adv are available\n",
    "if 'X_train_adv' not in locals() or 'y_train_adv' not in locals() or \\\n",
    "   'X_val_adv' not in locals() or 'y_val_adv' not in locals():\n",
    "    print(\"Warning: Activity training/validation data not found. Using dummy data for model_adv.\")\n",
    "    # Create dummy data to allow notebook to be 'runnable'\n",
    "    dummy_train_samples = 85 \n",
    "    dummy_val_samples = 15   \n",
    "    dummy_timesteps = X_train_activity_raw.shape[1] if 'X_train_activity_raw' in locals() else 50\n",
    "    dummy_features = 6\n",
    "    dummy_classes = 6\n",
    "\n",
    "    X_train_adv = np.random.rand(dummy_train_samples, dummy_timesteps, dummy_features)\n",
    "    _y_temp_train = np.random.randint(0, dummy_classes, size=(dummy_train_samples))\n",
    "    y_train_adv = np.eye(dummy_classes)[_y_temp_train]\n",
    "    \n",
    "    X_val_adv = np.random.rand(dummy_val_samples, dummy_timesteps, dummy_features)\n",
    "    _y_temp_val = np.random.randint(0, dummy_classes, size=(dummy_val_samples))\n",
    "    y_val_adv = np.eye(dummy_classes)[_y_temp_val]\n",
    "    print(\"Created DUMMY X_train_adv, y_train_adv, X_val_adv, y_val_adv for activity model.\")\n",
    "\n",
    "# Construct the model & Set the parameters.\n",
    "input_size_adv = X_train_adv.shape[2] \n",
    "rnn_units_adv = 64      \n",
    "output_size_adv = y_train_adv.shape[1] \n",
    "\n",
    "model_adv = Model()\n",
    "model_adv.add(RNN(input_size=input_size_adv, rnn_units=rnn_units_adv, seed=42)) # Added seed for RNN\n",
    "model_adv.add(Dense(input_dim=rnn_units_adv, output_dim=output_size_adv, seed=43)) # Added seed for Dense\n",
    "model_adv.add(Activation(\"softmax\", None)) \n",
    "\n",
    "# Train the model\n",
    "model_adv.train(X_train_adv, y_train_adv, X_val_adv, y_val_adv, \n",
    "                epochs=50, learning_rate=0.008, batch_size=32, loss_function='cce')\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the training and validation losses for the activity model\n",
    "model_adv.plot_losses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]

[end of lab6_recurrent_neural_network/Lab6.ipynb]
