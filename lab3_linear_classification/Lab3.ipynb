{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Linear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # Added for loading CSVs in final workflow\n",
    "from sklearn.metrics import f1_score # For F1 score calculation\n",
    "from sklearn.model_selection import train_test_split # For robust data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Utility Functions (Data Splitting)\n",
    "\n",
    "A utility function to split data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_numpy(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"\n",
    "    Splits NumPy arrays X and y into training and validation sets.\n",
    "    Uses sklearn.model_selection.train_test_split for robustness.\n",
    "    \"\"\"\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"X and y must have the same number of samples.\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y if y is not None else None\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. StandardizeData Function\n",
    "\n",
    "This function standardizes features by removing the mean and scaling to unit variance. The mean and standard deviation are calculated *only* from the training data (`X_train`) and then applied to all three datasets (`X_train`, `X_val`, `X_test`). This prevents data leakage from the validation and test sets into the training process.\n",
    "\n",
    "**Standardization Formula:**\n",
    "Z = (X - mean_train) / (std_train + epsilon)\n",
    "\n",
    "Where:\n",
    "- `X` is the data to be standardized (a feature column).\n",
    "- `mean_train` is the mean of the feature in the training data.\n",
    "- `std_train` is the standard deviation of the feature in the training data.\n",
    "- `epsilon` is a small constant (e.g., 1e-8) added to the standard deviation to prevent division by zero in case a feature has zero variance in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardizeData(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Standardizes the training, validation, and test datasets based on the \n",
    "    mean and standard deviation of the training data.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data features (samples x features).\n",
    "        X_val (np.ndarray): Validation data features (samples x features).\n",
    "        X_test (np.ndarray): Test data features (samples x features).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train_standardized, X_val_standardized, X_test_standardized)\n",
    "               The standardized versions of the input datasets.\n",
    "    \"\"\"\n",
    "    if X_train.ndim == 1:\n",
    "        X_train = X_train.reshape(-1, 1)\n",
    "    if X_val.ndim == 1 and X_val.size > 0: X_val = X_val.reshape(-1, 1)\n",
    "    if X_test.ndim == 1 and X_test.size > 0: X_test = X_test.reshape(-1, 1)\n",
    "            \n",
    "    mean_train = np.mean(X_train, axis=0)\n",
    "    std_train = np.std(X_train, axis=0)\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    X_train_standardized = (X_train - mean_train) / (std_train + epsilon)\n",
    "    \n",
    "    # Check if X_val and X_test have features before trying to access shape[1]\n",
    "    if X_val.size > 0 and X_val.shape[1] == mean_train.shape[0]:\n",
    "        X_val_standardized = (X_val - mean_train) / (std_train + epsilon)\n",
    "    elif X_val.size == 0:\n",
    "        X_val_standardized = X_val # Return as is if empty\n",
    "    else:\n",
    "        print(f\"Warning: X_val feature count ({X_val.shape[1] if X_val.size > 0 else 'N/A'}) differs from X_train ({mean_train.shape[0]}). Returning original X_val.\")\n",
    "        X_val_standardized = X_val\n",
    "    \n",
    "    if X_test.size > 0 and X_test.shape[1] == mean_train.shape[0]:\n",
    "        X_test_standardized = (X_test - mean_train) / (std_train + epsilon)\n",
    "    elif X_test.size == 0:\n",
    "        X_test_standardized = X_test # Return as is if empty\n",
    "    else:\n",
    "        print(f\"Warning: X_test feature count ({X_test.shape[1] if X_test.size > 0 else 'N/A'}) differs from X_train ({mean_train.shape[0]}). Returning original X_test.\")\n",
    "        X_test_standardized = X_test\n",
    "    \n",
    "    return X_train_standardized, X_val_standardized, X_test_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for StandardizeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample_sd = np.array([[1, 10, 100], [2, 20, 200], [3, 30, 300], [4, 40, 400]], dtype=float)\n",
    "X_val_sample_sd = np.array([[5, 50, 500], [6, 60, 600]], dtype=float)\n",
    "X_test_sample_sd = np.array([[0, 0, 0], [7, 70, 700]], dtype=float)\n",
    "X_train_std_sd, X_val_std_sd, X_test_std_sd = StandardizeData(X_train_sample_sd.copy(), X_val_sample_sd.copy(), X_test_sample_sd.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron Model\n",
    "\n",
    "This section defines the `Perceptron` class for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Perceptron Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    X : np.ndarray (for shape determination in __init__)\n",
    "      Input data used to determine the number of features and initialize weights.\n",
    "      This is typically the training data X.\n",
    "    n_iter : int\n",
    "      Number of passes over the training dataset (epochs). Default is 1.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting. The first element is the bias term (w_0),\n",
    "      and the rest are the weights for each feature (w_1, w_2, ...).\n",
    "    errors_ : list\n",
    "      Number of misclassifications (updates) in each epoch. Populated by 'fit'.\n",
    "    n_iter : int\n",
    "      Stores the number of iterations specified for training.\n",
    "      \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y)\n",
    "      Fits the training data to learn model weights.\n",
    "    linear_combination(X)\n",
    "      Calculates the net input (weighted sum + bias).\n",
    "    predict(X)\n",
    "      Returns class labels after applying a step function to the net input.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, n_iter=1):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError(\"Input X must be a NumPy array for Perceptron initialization.\")\n",
    "        if X.ndim == 1:\n",
    "            X_used_for_shape = X.reshape(1, -1)\n",
    "        else:\n",
    "            X_used_for_shape = X\n",
    "            \n",
    "        self.w_ = np.zeros(1 + X_used_for_shape.shape[1]) # +1 for the bias term w_0\n",
    "        self.errors_ = []\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def linear_combination(self, X):\n",
    "        \"\"\"Calculate net input (weighted sum).\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X_proc = X.reshape(1, -1)\n",
    "        else:\n",
    "            X_proc = X\n",
    "            \n",
    "        if X_proc.shape[1] != (len(self.w_) - 1):\n",
    "            raise ValueError(f\"Number of features in X ({X_proc.shape[1]}) does not match number of weights ({len(self.w_) - 1}).\")\n",
    "            \n",
    "        return np.dot(X_proc, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step function.\n",
    "        \"\"\"\n",
    "        net_input = self.linear_combination(X)\n",
    "        predictions = np.where(net_input >= 0, 1, 0)\n",
    "        return predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values (0 or 1).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # print(f\"Initial weights: {self.w_}\") # Optional: for debugging\n",
    "        self.errors_ = [] \n",
    "\n",
    "        for epoch_num in range(self.n_iter):\n",
    "            errors_in_epoch = 0\n",
    "            for xi, target_label in zip(X, y):\n",
    "                prediction = self.predict(xi) \n",
    "                \n",
    "                if isinstance(prediction, np.ndarray) and prediction.ndim > 0:\n",
    "                    prediction_scalar = prediction[0]\n",
    "                else:\n",
    "                    prediction_scalar = prediction\n",
    "\n",
    "                update = target_label - prediction_scalar\n",
    "                \n",
    "                if update != 0:\n",
    "                    self.w_[1:] += update * xi\n",
    "                    self.w_[0] += update \n",
    "                    errors_in_epoch += 1\n",
    "            \n",
    "            self.errors_.append(errors_in_epoch)\n",
    "            # Optional: print epoch-wise updates for debugging\n",
    "            # print(f\"Errors in epoch {epoch_num + 1}: {errors_in_epoch}\")\n",
    "            # print(f\"Updated weights after epoch {epoch_num + 1}: {self.w_}\")\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for Perceptron (Initialization, Linear Combination, Prediction, and Fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_p_ex = np.array([[2.0, 3.0], [0.5, 1.5], [4.0, 0.0], [1.0, 1.0], [3.0, 2.0]])\n",
    "y_sample_p_ex = np.array([1, 0, 1, 0, 1]) \n",
    "perceptron_ex_fit = Perceptron(X=X_sample_p_ex, n_iter=3) \n",
    "perceptron_ex_fit.fit(X_sample_p_ex, y_sample_p_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fisher's Linear Discriminant Function\n",
    "\n",
    "This function computes the Fisher's Linear Discriminant vector `w` which optimally separates two classes.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Compute Mean Vectors**: Calculate the mean vector for each class (0 and 1).\n",
    "    - `m0 = mean(X[y == 0])`\n",
    "    - `m1 = mean(X[y == 1])`\n",
    "2.  **Compute Within-Class Scatter Matrices (S0, S1)**:\n",
    "    - For each class, calculate its scatter matrix. For class 0:\n",
    "      `s0_scatter = sum((x - m0)(x - m0)^T)` for all `x` in class 0.\n",
    "    - Similarly for `s1_scatter` for class 1 using `m1`.\n",
    "3.  **Compute Total Within-Class Scatter Matrix (S_W)**:\n",
    "    - `S_W = s0_scatter + s1_scatter`\n",
    "4.  **Compute Between-Class Scatter Matrix (S_B)**:\n",
    "    - `S_B = (m1 - m0)(m1 - m0)^T`\n",
    "    - (Note: `S_B` itself is not directly used to find `w` in the `inv(S_W) * (m1-m0)` formulation, but it's part of Fisher's criterion J(w) = (w^T S_B w) / (w^T S_W w). The direction that maximizes this is proportional to `inv(S_W) * (m1-m0)`).\n",
    "5.  **Compute Discriminant Vector (w)**:\n",
    "    - `w = inv(S_W) @ (m1 - m0)`\n",
    "    - If `S_W` is singular, its pseudo-inverse `pinv(S_W)` is used.\n",
    "6.  **Normalize Discriminant Vector**: \n",
    "    - `w = w / ||w||` (Euclidean norm)\n",
    "    This makes the vector a unit vector, which is common practice but doesn't change its direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_discriminant(X, y):\n",
    "    \"\"\"\n",
    "    Computes Fisher's Linear Discriminant vector w.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized input data (samples x features).\n",
    "        y (np.ndarray): Binary labels (0 or 1) for each sample.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized Fisher's Linear Discriminant vector w.\n",
    "                    Returns None if computation fails (e.g., singular S_W and pinv also fails, or class has no samples).\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    \n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # Separate data by class\n",
    "    X0 = X[y == 0]\n",
    "    X1 = X[y == 1]\n",
    "\n",
    "    if X0.size == 0 or X1.size == 0:\n",
    "        print(\"Warning: One or both classes have no samples. Cannot compute Fisher Discriminant.\")\n",
    "        return None\n",
    "\n",
    "    # Compute mean vectors\n",
    "    m0 = np.mean(X0, axis=0)\n",
    "    m1 = np.mean(X1, axis=0)\n",
    "\n",
    "    # Compute within-class scatter matrices (S0 and S1)\n",
    "    s0_scatter = np.zeros((num_features, num_features))\n",
    "    for row in X0:\n",
    "        diff = (row - m0).reshape(num_features, 1)\n",
    "        s0_scatter += np.dot(diff, diff.T)\n",
    "    \n",
    "    s1_scatter = np.zeros((num_features, num_features))\n",
    "    for row in X1:\n",
    "        diff = (row - m1).reshape(num_features, 1)\n",
    "        s1_scatter += np.dot(diff, diff.T)\n",
    "\n",
    "    # Total within-class scatter matrix S_W\n",
    "    S_W = s0_scatter + s1_scatter\n",
    "\n",
    "    # Compute discriminant vector w\n",
    "    try:\n",
    "        S_W_inv = np.linalg.inv(S_W)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: S_W is singular, using pseudo-inverse.\")\n",
    "        try:\n",
    "            S_W_inv = np.linalg.pinv(S_W)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Error: Pseudo-inverse of S_W also failed. Cannot compute w.\")\n",
    "            return None\n",
    "            \n",
    "    w = S_W_inv @ (m1 - m0)\n",
    "    \n",
    "    # Normalize discriminant vector w\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    if norm_w == 0:\n",
    "        print(\"Warning: Norm of w is zero. Cannot normalize.\")\n",
    "        return w # Or None, or handle as appropriate\n",
    "    w_normalized = w / norm_w\n",
    "    \n",
    "    return w_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for fisher_discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample standardized data for Fisher's LDA\n",
    "X_exp_fisher = np.array([\n",
    "    [-1, -1], [-1.5, -0.5], [-0.5, -1.5], [-0.8, -0.8], # Class 0\n",
    "    [ 1,  1], [ 1.5,  0.5], [ 0.5,  1.5], [ 0.8,  0.8]  # Class 1\n",
    "])\n",
    "y_exp_fisher = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "w_fisher_example = fisher_discriminant(X_exp_fisher, y_exp_fisher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fisher's LDA Decision Boundary Calculation\n",
    "\n",
    "Once data is projected onto the 1D space defined by Fisher's discriminant vector `w`, a decision boundary is needed to classify new points. This boundary is typically chosen as the midpoint between the means of the two classes in this projected 1D space.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Project Data**: The training data `X_train` is projected onto `w`: `X_train_lda = X_train @ w`.\n",
    "2.  **Calculate Means of Projected Classes**: \n",
    "    - `mean_class_0_lda = mean(X_train_lda[y_train == 0])`\n",
    "    - `mean_class_1_lda = mean(X_train_lda[y_train == 1])`\n",
    "3.  **Compute Decision Boundary**: \n",
    "    - `decision_boundary = (mean_class_0_lda + mean_class_1_lda) / 2.0`\n",
    "    This boundary value is a scalar in the 1D projected space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_calculation(X_train_lda, y_train):\n",
    "    \"\"\"\n",
    "    Calculates the decision boundary for 1D LDA-projected data.\n",
    "\n",
    "    Args:\n",
    "        X_train_lda (np.ndarray): Training data projected onto the LDA vector (1D array).\n",
    "        y_train (np.ndarray): Binary labels (0 or 1) for the training data.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated decision boundary.\n",
    "               Returns np.nan if one or both classes are empty in the training data.\n",
    "    \"\"\"\n",
    "    if X_train_lda.ndim != 1:\n",
    "        if X_train_lda.shape[1] == 1:\n",
    "            X_train_lda = X_train_lda.flatten()\n",
    "        else:\n",
    "            raise ValueError(\"X_train_lda must be a 1D array or a 2D array with one column.\")\n",
    "\n",
    "    class_0_projected = X_train_lda[y_train == 0]\n",
    "    class_1_projected = X_train_lda[y_train == 1]\n",
    "\n",
    "    if class_0_projected.size == 0:\n",
    "        # print(\"Warning: Class 0 has no samples in X_train_lda. Cannot compute its mean.\")\n",
    "        mean_class_0_lda = np.nan\n",
    "    else:\n",
    "        mean_class_0_lda = np.mean(class_0_projected)\n",
    "        \n",
    "    if class_1_projected.size == 0:\n",
    "        # print(\"Warning: Class 1 has no samples in X_train_lda. Cannot compute its mean.\")\n",
    "        mean_class_1_lda = np.nan\n",
    "    else:\n",
    "        mean_class_1_lda = np.mean(class_1_projected)\n",
    "\n",
    "    if np.isnan(mean_class_0_lda) or np.isnan(mean_class_1_lda):\n",
    "        # print(\"Warning: Mean for one or both classes is NaN. Boundary calculation failed.\")\n",
    "        return np.nan\n",
    "        \n",
    "    decision_boundary = (mean_class_0_lda + mean_class_1_lda) / 2.0\n",
    "    \n",
    "    return decision_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for boundary_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'w_fisher_example' in globals() and w_fisher_example is not None:\n",
    "    if X_exp_fisher.shape[1] == len(w_fisher_example):\n",
    "        X_exp_lda_bc = X_exp_fisher @ w_fisher_example \n",
    "        boundary_example_bc = boundary_calculation(X_exp_lda_bc, y_exp_fisher)\n",
    "    # else: print(\"Skipping boundary_calculation example due to shape mismatch.\")\n",
    "# else:\n",
    "    # print(\"Skipping boundary_calculation example as w_fisher_example is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LDA Classifier Function\n",
    "\n",
    "This function uses Fisher's LDA to classify test data. It first computes the discriminant vector `W` from the training data, projects both training and test data onto `W`, calculates a decision boundary from the projected training data, and then classifies the projected test data based on this boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_classifier(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Classifies test data using Fisher's Linear Discriminant Analysis.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Standardized training data (samples x features).\n",
    "        y_train (np.ndarray): Binary labels (0 or 1) for X_train.\n",
    "        X_test (np.ndarray): Standardized test data (samples x features).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted labels (0 or 1) for X_test. \n",
    "                    Returns an empty array if classification cannot proceed.\n",
    "    \"\"\"\n",
    "    if X_train.ndim == 1:\n",
    "        X_train = X_train.reshape(-1,1)\n",
    "    if X_test.ndim == 1:\n",
    "        X_test = X_test.reshape(-1,1)\n",
    "    \n",
    "    W = fisher_discriminant(X_train, y_train)\n",
    "    if W is None:\n",
    "        # print(\"LDA Classifier Error: Could not compute discriminant vector W.\")\n",
    "        return np.array([]) \n",
    "\n",
    "    if W.ndim > 1 and W.shape[1] == 1:\n",
    "        W = W.flatten()\n",
    "    elif W.ndim > 1:\n",
    "        # print(f\"LDA Classifier Error: Discriminant vector W has unexpected shape {W.shape}.\")\n",
    "        return np.array([])\n",
    "        \n",
    "    X_train_lda = X_train @ W\n",
    "    X_test_lda = X_test @ W\n",
    "\n",
    "    decision_boundary = boundary_calculation(X_train_lda, y_train)\n",
    "    if np.isnan(decision_boundary):\n",
    "        # print(\"LDA Classifier Error: Could not compute decision boundary (resulted in NaN).\")\n",
    "        return np.array([]) \n",
    "\n",
    "    mean_class_0_lda = np.mean(X_train_lda[y_train == 0]) if X_train_lda[y_train == 0].size > 0 else np.nan\n",
    "    mean_class_1_lda = np.mean(X_train_lda[y_train == 1]) if X_train_lda[y_train == 1].size > 0 else np.nan\n",
    "\n",
    "    if np.isnan(mean_class_0_lda) or np.isnan(mean_class_1_lda):\n",
    "         # print(\"LDA Classifier Error: Mean of one or both classes in projected space is NaN.\")\n",
    "         return np.array([])\n",
    "\n",
    "    if mean_class_1_lda >= mean_class_0_lda:\n",
    "        y_pred = np.where(X_test_lda >= decision_boundary, 1, 0)\n",
    "    else:\n",
    "        y_pred = np.where(X_test_lda < decision_boundary, 1, 0) \n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for lda_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda_cl_example = X_exp_fisher\n",
    "y_train_lda_cl_example = y_exp_fisher\n",
    "X_test_lda_cl_example = np.array([\n",
    "    [-2, -2], [ 2,  2], [ 0,  0], [-0.5, 1], [ 1, -0.5]\n",
    "])\n",
    "y_predictions_lda_cl = lda_classifier(X_train_lda_cl_example, y_train_lda_cl_example, X_test_lda_cl_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Mean, Variance, and Prior for LDA-Projected Data\n",
    "\n",
    "This function calculates the mean, variance, and prior probability for each class from the 1D LDA-projected training data. These statistics are essential for building a Gaussian Naive Bayes classifier or a Quadratic Discriminant Analysis (QDA) classifier on the projected data.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Separate Projected Data by Class**: Given `X_train_lda` (1D array of projected training samples) and `y_train` (corresponding labels):\n",
    "    - `class_0_data = X_train_lda[y_train == 0]`\n",
    "    - `class_1_data = X_train_lda[y_train == 1]`\n",
    "2.  **Calculate Means**: \n",
    "    - `mean_class_0 = np.mean(class_0_data)` (or 0 if `class_0_data` is empty).\n",
    "    - `mean_class_1 = np.mean(class_1_data)` (or 0 if `class_1_data` is empty).\n",
    "3.  **Calculate Variances**:\n",
    "    - `variance_class_0 = np.var(class_0_data)` (or 1 if `class_0_data` has < 2 samples).\n",
    "    - `variance_class_1 = np.var(class_1_data)` (or 1 if `class_1_data` has < 2 samples).\n",
    "    The default variance of 1 for small samples or empty classes helps prevent issues like zero variance in likelihood calculations.\n",
    "4.  **Calculate Priors**:\n",
    "    - `n_total = len(y_train)`\n",
    "    - `prior_class_0 = len(class_0_data) / n_total` (or 0 if `n_total` is 0).\n",
    "    - `prior_class_1 = len(class_1_data) / n_total` (or 0 if `n_total` is 0).\n",
    "5.  **Return**: The function returns these six values: `mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_variance_prior(X_train_lda, y_train):\n",
    "    \"\"\"\n",
    "    Calculates mean, variance, and prior for each class from 1D LDA-projected data.\n",
    "\n",
    "    Args:\n",
    "        X_train_lda (np.ndarray): Training data projected onto the LDA vector (1D array).\n",
    "        y_train (np.ndarray): Binary labels (0 or 1) for the training data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mean0, var0, prior0, mean1, var1, prior1)\n",
    "    \"\"\"\n",
    "    if X_train_lda.ndim != 1:\n",
    "        if X_train_lda.shape[1] == 1:\n",
    "            X_train_lda = X_train_lda.flatten()\n",
    "        else:\n",
    "            raise ValueError(\"X_train_lda must be a 1D array or a 2D array with one column.\")\n",
    "\n",
    "    if X_train_lda.size == 0 or y_train.size == 0 or X_train_lda.size != y_train.size:\n",
    "        # print(\"Warning: Input data is empty or sizes mismatch. Returning default stats.\")\n",
    "        return 0, 1, 0, 0, 1, 0 # Default mean=0, var=1, prior=0\n",
    "\n",
    "    class_0_data = X_train_lda[y_train == 0]\n",
    "    class_1_data = X_train_lda[y_train == 1]\n",
    "\n",
    "    mean_class_0 = np.mean(class_0_data) if class_0_data.size > 0 else 0.0\n",
    "    mean_class_1 = np.mean(class_1_data) if class_1_data.size > 0 else 0.0\n",
    "\n",
    "    variance_class_0 = np.var(class_0_data, ddof=0) if class_0_data.size >= 2 else 1.0 \n",
    "    if class_0_data.size == 1: variance_class_0 = 1.0 \n",
    "    elif class_0_data.size == 0: variance_class_0 = 1.0\n",
    "        \n",
    "    variance_class_1 = np.var(class_1_data, ddof=0) if class_1_data.size >= 2 else 1.0\n",
    "    if class_1_data.size == 1: variance_class_1 = 1.0\n",
    "    elif class_1_data.size == 0: variance_class_1 = 1.0\n",
    "\n",
    "    n_total = len(y_train)\n",
    "    prior_class_0 = class_0_data.size / n_total if n_total > 0 else 0.0\n",
    "    prior_class_1 = class_1_data.size / n_total if n_total > 0 else 0.0\n",
    "\n",
    "    return mean_class_0, variance_class_0, prior_class_0, mean_class_1, variance_class_1, prior_class_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for mean_variance_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_exp_fisher' in globals() and 'y_exp_fisher' in globals() and 'w_fisher_example' in globals() and w_fisher_example is not None:\n",
    "    current_X_exp_lda_mvp = X_exp_fisher @ w_fisher_example \n",
    "    current_y_exp_fisher_mvp = y_exp_fisher\n",
    "    m0_mvp, v0_mvp, p0_mvp, m1_mvp, v1_mvp, p1_mvp = mean_variance_prior(current_X_exp_lda_mvp, current_y_exp_fisher_mvp)\n",
    "    # print(f\"Class 0 (MVP Example): Mean={m0_mvp:.4f}, Variance={v0_mvp:.4f}, Prior={p0_mvp:.4f}\")\n",
    "    # print(f\"Class 1 (MVP Example): Mean={m1_mvp:.4f}, Variance={v1_mvp:.4f}, Prior={p1_mvp:.4f}\")\n",
    "# else:\n",
    "    # print(\"Skipping mean_variance_prior example as prerequisite variables are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gaussian Likelihood Function\n",
    "\n",
    "This function calculates the likelihood of a data point `x` belonging to a class, assuming the class's data distribution is Gaussian (normal). This is a key component in Gaussian Naive Bayes and QDA/LDA classifiers.\n",
    "\n",
    "**Gaussian Probability Density Function (PDF) Formula:**\n",
    "P(x | C_k) = (1 / sqrt(2 * pi * sigma_k^2)) * exp(-((x - mu_k)^2) / (2 * sigma_k^2))\n",
    "\n",
    "Where:\n",
    "- `x` is the data point (a scalar value in the 1D projected space).\n",
    "- `mu_k` is the mean of class C_k in the projected space.\n",
    "- `sigma_k^2` is the variance of class C_k in the projected space.\n",
    "- `pi` is the mathematical constant pi.\n",
    "\n",
    "To avoid numerical issues (e.g., division by zero if variance is very small or zero), a small epsilon (e.g., 1e-9) is added to the variance terms in the denominator and under the square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(mean, variance, x):\n",
    "    \"\"\"\n",
    "    Calculates the Gaussian probability density (likelihood) of a data point x.\n",
    "\n",
    "    Args:\n",
    "        mean (float): Mean of the Gaussian distribution.\n",
    "        variance (float): Variance of the Gaussian distribution.\n",
    "        x (float or np.ndarray): The data point(s) for which to calculate the likelihood.\n",
    "\n",
    "    Returns:\n",
    "        float or np.ndarray: The Gaussian probability density value(s).\n",
    "    \"\"\"\n",
    "    epsilon = 1e-9 \n",
    "    stable_variance = variance + epsilon\n",
    "    exponent = -((x - mean)**2) / (2 * stable_variance)\n",
    "    coefficient = 1 / (np.sqrt(2 * np.pi * stable_variance))\n",
    "    prob_density = coefficient * np.exp(exponent)\n",
    "    return prob_density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ex_lh = 0.0; variance_ex_lh = 1.0; x_point_ex_lh = 0.0\n",
    "pdf_val_lh = likelihood(mean_ex_lh, variance_ex_lh, x_point_ex_lh)\n",
    "# print(f\"Likelihood for x={x_point_ex_lh}, mean={mean_ex_lh}, variance={variance_ex_lh}: {pdf_val_lh:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LDA Classifier with MAP Rule\n",
    "\n",
    "This function classifies test data using Fisher's LDA followed by a Maximum A Posteriori (MAP) decision rule. It calculates class-conditional probabilities (likelihoods) using Gaussian distributions for the projected data and combines them with class priors to determine the posterior probabilities.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Compute Discriminant Vector `W`**: Same as `lda_classifier` (using `fisher_discriminant`).\n",
    "2.  **Project Data**: Project `X_train` and `X_test` onto `W` to get `X_train_lda` and `X_test_lda`.\n",
    "3.  **Calculate Class Statistics and Priors**: Use `mean_variance_prior(X_train_lda, y_train)` to get `mean0, var0, prior0, mean1, var1, prior1`.\n",
    "4.  **Classify Test Data (MAP Rule)**: For each `x_sample` in `X_test_lda`:\n",
    "    *   Calculate likelihood for class 0: `lh0 = likelihood(mean0, var0, x_sample)`.\n",
    "    *   Calculate likelihood for class 1: `lh1 = likelihood(mean1, var1, x_sample)`.\n",
    "    *   Calculate posterior for class 0: `post0 = lh0 * prior0`.\n",
    "    *   Calculate posterior for class 1: `post1 = lh1 * prior1`.\n",
    "    *   Predict class 1 if `post1 >= post0`, otherwise predict class 0.\n",
    "5.  **Return Predictions**: A NumPy array of predicted labels for `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_classifier_map(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Classifies test data using Fisher's LDA with a MAP decision rule.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Standardized training data (samples x features).\n",
    "        y_train (np.ndarray): Binary labels (0 or 1) for X_train.\n",
    "        X_test (np.ndarray): Standardized test data (samples x features).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted labels (0 or 1) for X_test.\n",
    "                    Returns an empty array if classification cannot proceed.\n",
    "    \"\"\"\n",
    "    if X_train.ndim == 1: X_train = X_train.reshape(-1,1)\n",
    "    if X_test.ndim == 1 and X_test.size > 0: X_test = X_test.reshape(-1,1)\n",
    "    elif X_test.ndim == 1 and X_test.size == 0: X_test = np.array([]).reshape(0,X_train.shape[1]) # Match feature dim if empty\n",
    "\n",
    "    W = fisher_discriminant(X_train, y_train)\n",
    "    if W is None:\n",
    "        print(\"LDA MAP Classifier Error: Could not compute discriminant vector W.\")\n",
    "        return np.array([])\n",
    "\n",
    "    if W.ndim > 1 and W.shape[1] == 1: W = W.flatten()\n",
    "    elif W.ndim > 1:\n",
    "        print(f\"LDA MAP Classifier Error: Discriminant vector W has unexpected shape {W.shape}.\")\n",
    "        return np.array([])\n",
    "        \n",
    "    X_train_lda = X_train @ W\n",
    "    X_test_lda = X_test @ W # This will be 1D array or scalar if X_test is single sample\n",
    "\n",
    "    mean0, var0, prior0, mean1, var1, prior1 = mean_variance_prior(X_train_lda, y_train)\n",
    "    \n",
    "    # Handle cases where a class might be absent in training, leading to prior=0 or NaN stats\n",
    "    if prior0 == 0 and prior1 == 0: # Both classes absent, should not happen if fisher_discriminant passed\n",
    "        print(\"LDA MAP Classifier Error: Both class priors are zero. Cannot classify.\")\n",
    "        return np.array([0] * len(X_test_lda)) # Default prediction or empty\n",
    "    if np.isnan(mean0) or np.isnan(var0) or np.isnan(mean1) or np.isnan(var1):\n",
    "        print(\"LDA MAP Classifier Error: Class statistics (mean/var) are NaN. Cannot classify.\")\n",
    "        return np.array([0] * len(X_test_lda))\n",
    "        \n",
    "    predictions = []\n",
    "    for x_sample_lda in X_test_lda:\n",
    "        lh0 = likelihood(mean0, var0, x_sample_lda) if prior0 > 0 else 0\n",
    "        lh1 = likelihood(mean1, var1, x_sample_lda) if prior1 > 0 else 0\n",
    "        \n",
    "        post0 = lh0 * prior0\n",
    "        post1 = lh1 * prior1\n",
    "        \n",
    "        if post1 >= post0:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "            \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for lda_classifier_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using X_exp_fisher and y_exp_fisher from the fisher_discriminant example as training data\n",
    "X_train_map_example = X_exp_fisher\n",
    "y_train_map_example = y_exp_fisher\n",
    "\n",
    "# Using X_test_lda_example from lda_classifier example\n",
    "X_test_map_example = X_test_lda_cl_example # This was the name in lda_classifier example section\n",
    "\n",
    "print(\"LDA MAP Classifier Example:\")\n",
    "# print(\"X_train:\\n\", X_train_map_example)\n",
    "# print(\"y_train:\\n\", y_train_map_example)\n",
    "# print(\"X_test:\\n\", X_test_map_example)\n",
    "\n",
    "y_predictions_lda_map = lda_classifier_map(X_train_map_example, y_train_map_example, X_test_map_example)\n",
    "\n",
    "if y_predictions_lda_map is not None and y_predictions_lda_map.size > 0:\n",
    "    print(\"\\nPredicted labels for X_test (MAP rule):\", y_predictions_lda_map)\n",
    "    # Expected for X_test_lda_example = [[-2,-2], [2,2], [0,0], [-0.5,1], [1,-0.5]]\n",
    "    # Given priors are 0.5/0.5, this should be similar to lda_classifier if boundary is at 0.\n",
    "    # For this data, m0_proj ~ -1.34, m1_proj ~ 1.34. Variances are small and equal.\n",
    "    # For x_proj = 0, likelihoods should be equal, posteriors equal, predict 1 (due to >=).\n",
    "    # Expected (approximate): [0, 1, 1, ?, ?]\n",
    "else:\n",
    "    print(\"LDA MAP classification failed or returned no predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3: Full Workflow and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Datasets (`lab3_training.csv`, `lab3_testing.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    training_df = pd.read_csv('lab3_training.csv')\n",
    "    testing_df = pd.read_csv('lab3_testing.csv')\n",
    "    print(\"Successfully loaded lab3_training.csv and lab3_testing.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"CSV files not found. Creating dummy data for final workflow.\")\n",
    "    # Dummy training_df: 100 rows, 3 features + 'label'\n",
    "    training_data_dict = {\n",
    "        'feature1': np.random.rand(100) * 10 - 5, \n",
    "        'feature2': np.random.rand(100) * 20 - 10,\n",
    "        'feature3': np.random.normal(0, 1, 100),\n",
    "        'label': np.random.randint(0, 2, 100)\n",
    "    }\n",
    "    training_df = pd.DataFrame(training_data_dict)\n",
    "    # Ensure some class balance for dummy data if needed\n",
    "    training_df['label'].iloc[:50] = 0\n",
    "    training_df['label'].iloc[50:] = 1\n",
    "    \n",
    "    # Dummy testing_df: 50 rows, 3 features (no 'Id' or 'label')\n",
    "    testing_data_dict = {\n",
    "        'feature1': np.random.rand(50) * 10 - 5,\n",
    "        'feature2': np.random.rand(50) * 20 - 10,\n",
    "        'feature3': np.random.normal(0, 1, 50)\n",
    "    }\n",
    "    testing_df = pd.DataFrame(testing_data_dict)\n",
    "    print(\"Created dummy training and testing DataFrames.\")\n",
    "\n",
    "# print(\"Training Data Head:\\n\", training_df.head())\n",
    "# print(\"Testing Data Head:\\n\", testing_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Training Data for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = training_df.drop(columns=['label']).values\n",
    "y_all = training_df['label'].values\n",
    "X_test_final_loaded = testing_df.values # This is the test set for final predictions\n",
    "\n",
    "# Using the utility function defined earlier\n",
    "X_train_full, X_val_full, y_train_full, y_val_full = split_data_numpy(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X_train_full shape: {X_train_full.shape}, y_train_full shape: {y_train_full.shape}\")\n",
    "print(f\"X_val_full shape: {X_val_full.shape}, y_val_full shape: {y_val_full.shape}\")\n",
    "print(f\"X_test_final_loaded shape: {X_test_final_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std, X_val_std, X_test_std = StandardizeData(X_train_full, X_val_full, X_test_final_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Part 1: Perceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training Perceptron ---\")\n",
    "perceptron_model = Perceptron(X=X_train_std, n_iter=100) # Using 100 iterations\n",
    "perceptron_model.fit(X_train_std, y_train_full)\n",
    "\n",
    "y_pred_perceptron_val = perceptron_model.predict(X_val_std)\n",
    "f1_perceptron = f1_score(y_val_full, y_pred_perceptron_val, zero_division=0)\n",
    "print(f\"Perceptron F1 Score on Validation Set: {f1_perceptron:.4f}\")\n",
    "\n",
    "y_pred_perceptron_test = perceptron_model.predict(X_test_std)\n",
    "output_df_part1 = pd.DataFrame({'Id': np.arange(1, len(y_pred_perceptron_test) + 1), 'Label': y_pred_perceptron_test})\n",
    "output_df_part1.to_csv('lab3_part1.csv', index=False)\n",
    "print(f\"Perceptron test predictions saved to lab3_part1.csv. Shape: {output_df_part1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Part 2: LDA Classifier (using Fisher's Discriminant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training LDA Classifier ---\")\n",
    "y_pred_lda_val = lda_classifier(X_train_std, y_train_full, X_val_std)\n",
    "if y_pred_lda_val.size > 0:\n",
    "    f1_lda = f1_score(y_val_full, y_pred_lda_val, zero_division=0)\n",
    "    print(f\"LDA Classifier F1 Score on Validation Set: {f1_lda:.4f}\")\n",
    "else:\n",
    "    print(\"LDA Classifier validation failed, F1 score not calculated.\")\n",
    "\n",
    "y_pred_lda_test = lda_classifier(X_train_std, y_train_full, X_test_std)\n",
    "if y_pred_lda_test.size > 0:\n",
    "    output_df_part2 = pd.DataFrame({'Id': np.arange(1, len(y_pred_lda_test) + 1), 'Label': y_pred_lda_test})\n",
    "    output_df_part2.to_csv('lab3_part2.csv', index=False)\n",
    "    print(f\"LDA Classifier test predictions saved to lab3_part2.csv. Shape: {output_df_part2.shape}\")\n",
    "else:\n",
    "    print(\"LDA Classifier test prediction failed. File not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Part 3: LDA Classifier with MAP Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training LDA Classifier with MAP Rule ---\")\n",
    "y_pred_lda_map_val = lda_classifier_map(X_train_std, y_train_full, X_val_std)\n",
    "if y_pred_lda_map_val.size > 0:\n",
    "    f1_lda_map = f1_score(y_val_full, y_pred_lda_map_val, zero_division=0)\n",
    "    print(f\"LDA MAP Classifier F1 Score on Validation Set: {f1_lda_map:.4f}\")\n",
    "else:\n",
    "    print(\"LDA MAP Classifier validation failed, F1 score not calculated.\")\n",
    "\n",
    "y_pred_lda_map_test = lda_classifier_map(X_train_std, y_train_full, X_test_std)\n",
    "if y_pred_lda_map_test.size > 0:\n",
    "    output_df_part3 = pd.DataFrame({'Id': np.arange(1, len(y_pred_lda_map_test) + 1), 'Label': y_pred_lda_map_test})\n",
    "    output_df_part3.to_csv('lab3_part3.csv', index=False)\n",
    "    print(f\"LDA MAP Classifier test predictions saved to lab3_part3.csv. Shape: {output_df_part3.shape}\")\n",
    "else:\n",
    "    print(\"LDA MAP Classifier test prediction failed. File not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
