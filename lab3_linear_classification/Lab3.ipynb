{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Linear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. StandardizeData Function\n",
    "\n",
    "This function standardizes features by removing the mean and scaling to unit variance. The mean and standard deviation are calculated *only* from the training data (`X_train`) and then applied to all three datasets (`X_train`, `X_val`, `X_test`). This prevents data leakage from the validation and test sets into the training process.\n",
    "\n",
    "**Standardization Formula:**\n",
    "Z = (X - mean_train) / (std_train + epsilon)\n",
    "\n",
    "Where:\n",
    "- `X` is the data to be standardized (a feature column).\n",
    "- `mean_train` is the mean of the feature in the training data.\n",
    "- `std_train` is the standard deviation of the feature in the training data.\n",
    "- `epsilon` is a small constant (e.g., 1e-8) added to the standard deviation to prevent division by zero in case a feature has zero variance in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardizeData(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Standardizes the training, validation, and test datasets based on the \n",
    "    mean and standard deviation of the training data.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Training data features (samples x features).\n",
    "        X_val (np.ndarray): Validation data features (samples x features).\n",
    "        X_test (np.ndarray): Test data features (samples x features).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train_standardized, X_val_standardized, X_test_standardized)\n",
    "               The standardized versions of the input datasets.\n",
    "    \"\"\"\n",
    "    if X_train.ndim == 1:\n",
    "        X_train = X_train.reshape(-1, 1)\n",
    "        if X_val.ndim == 1: X_val = X_val.reshape(-1, 1)\n",
    "        if X_test.ndim == 1: X_test = X_test.reshape(-1, 1)\n",
    "            \n",
    "    mean_train = np.mean(X_train, axis=0)\n",
    "    std_train = np.std(X_train, axis=0)\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    X_train_standardized = (X_train - mean_train) / (std_train + epsilon)\n",
    "    \n",
    "    if X_val.shape[1] == mean_train.shape[0]:\n",
    "        X_val_standardized = (X_val - mean_train) / (std_train + epsilon)\n",
    "    else:\n",
    "        print(f\"Warning: X_val feature count ({X_val.shape[1]}) differs from X_train ({mean_train.shape[0]}). Returning original X_val.\")\n",
    "        X_val_standardized = X_val\n",
    "    \n",
    "    if X_test.shape[1] == mean_train.shape[0]:\n",
    "        X_test_standardized = (X_test - mean_train) / (std_train + epsilon)\n",
    "    else:\n",
    "        print(f\"Warning: X_test feature count ({X_test.shape[1]}) differs from X_train ({mean_train.shape[0]}). Returning original X_test.\")\n",
    "        X_test_standardized = X_test\n",
    "    \n",
    "    return X_train_standardized, X_val_standardized, X_test_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for StandardizeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample_sd = np.array([[1, 10, 100], [2, 20, 200], [3, 30, 300], [4, 40, 400]], dtype=float)\n",
    "X_val_sample_sd = np.array([[5, 50, 500], [6, 60, 600]], dtype=float)\n",
    "X_test_sample_sd = np.array([[0, 0, 0], [7, 70, 700]], dtype=float)\n",
    "X_train_std_sd, X_val_std_sd, X_test_std_sd = StandardizeData(X_train_sample_sd.copy(), X_val_sample_sd.copy(), X_test_sample_sd.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron Model\n",
    "\n",
    "This section defines the `Perceptron` class for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Perceptron Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    X : np.ndarray (for shape determination in __init__)\n",
    "      Input data used to determine the number of features and initialize weights.\n",
    "      This is typically the training data X.\n",
    "    n_iter : int\n",
    "      Number of passes over the training dataset (epochs). Default is 1.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting. The first element is the bias term (w_0),\n",
    "      and the rest are the weights for each feature (w_1, w_2, ...).\n",
    "    errors_ : list\n",
    "      Number of misclassifications (updates) in each epoch. Populated by 'fit'.\n",
    "    n_iter : int\n",
    "      Stores the number of iterations specified for training.\n",
    "      \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y)\n",
    "      Fits the training data to learn model weights.\n",
    "    linear_combination(X)\n",
    "      Calculates the net input (weighted sum + bias).\n",
    "    predict(X)\n",
    "      Returns class labels after applying a step function to the net input.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, n_iter=1):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            raise ValueError(\"Input X must be a NumPy array for Perceptron initialization.\")\n",
    "        if X.ndim == 1:\n",
    "            X_used_for_shape = X.reshape(1, -1)\n",
    "        else:\n",
    "            X_used_for_shape = X\n",
    "            \n",
    "        self.w_ = np.zeros(1 + X_used_for_shape.shape[1]) # +1 for the bias term w_0\n",
    "        self.errors_ = []\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def linear_combination(self, X):\n",
    "        \"\"\"Calculate net input (weighted sum).\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X_proc = X.reshape(1, -1)\n",
    "        else:\n",
    "            X_proc = X\n",
    "            \n",
    "        if X_proc.shape[1] != (len(self.w_) - 1):\n",
    "            raise ValueError(f\"Number of features in X ({X_proc.shape[1]}) does not match number of weights ({len(self.w_) - 1}).\")\n",
    "            \n",
    "        return np.dot(X_proc, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step function.\n",
    "        \"\"\"\n",
    "        net_input = self.linear_combination(X)\n",
    "        predictions = np.where(net_input >= 0, 1, 0)\n",
    "        return predictions\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values (0 or 1).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # print(f\"Initial weights: {self.w_}\") # Optional: for debugging\n",
    "        self.errors_ = [] \n",
    "\n",
    "        for epoch_num in range(self.n_iter):\n",
    "            errors_in_epoch = 0\n",
    "            for xi, target_label in zip(X, y):\n",
    "                prediction = self.predict(xi) \n",
    "                \n",
    "                if isinstance(prediction, np.ndarray) and prediction.ndim > 0:\n",
    "                    prediction_scalar = prediction[0]\n",
    "                else:\n",
    "                    prediction_scalar = prediction\n",
    "\n",
    "                update = target_label - prediction_scalar\n",
    "                \n",
    "                if update != 0:\n",
    "                    self.w_[1:] += update * xi\n",
    "                    self.w_[0] += update \n",
    "                    errors_in_epoch += 1\n",
    "            \n",
    "            self.errors_.append(errors_in_epoch)\n",
    "            # Optional: print epoch-wise updates for debugging\n",
    "            # print(f\"Errors in epoch {epoch_num + 1}: {errors_in_epoch}\")\n",
    "            # print(f\"Updated weights after epoch {epoch_num + 1}: {self.w_}\")\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for Perceptron (Initialization, Linear Combination, Prediction, and Fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_p_ex = np.array([[2.0, 3.0], [0.5, 1.5], [4.0, 0.0], [1.0, 1.0], [3.0, 2.0]])\n",
    "y_sample_p_ex = np.array([1, 0, 1, 0, 1]) \n",
    "perceptron_ex_fit = Perceptron(X=X_sample_p_ex, n_iter=3) \n",
    "# print(f\"Initial weights: {perceptron_ex_fit.w_}\")\n",
    "perceptron_ex_fit.fit(X_sample_p_ex, y_sample_p_ex)\n",
    "# print(f\"Errors per epoch: {perceptron_ex_fit.errors_}\")\n",
    "# print(f\"Final weights: {perceptron_ex_fit.w_}\")\n",
    "# predictions_after_fit_ex = perceptron_ex_fit.predict(X_sample_p_ex)\n",
    "# print(f\"Predictions on X_sample_p_ex after fitting: {predictions_after_fit_ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fisher's Linear Discriminant Function\n",
    "\n",
    "This function computes the Fisher's Linear Discriminant vector `w` which optimally separates two classes.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Compute Mean Vectors**: Calculate the mean vector for each class (0 and 1).\n",
    "    - `m0 = mean(X[y == 0])`\n",
    "    - `m1 = mean(X[y == 1])`\n",
    "2.  **Compute Within-Class Scatter Matrices (S0, S1)**:\n",
    "    - For each class, calculate its scatter matrix. For class 0:\n",
    "      `s0_scatter = sum((x - m0)(x - m0)^T)` for all `x` in class 0.\n",
    "    - Similarly for `s1_scatter` for class 1 using `m1`.\n",
    "3.  **Compute Total Within-Class Scatter Matrix (S_W)**:\n",
    "    - `S_W = s0_scatter + s1_scatter`\n",
    "4.  **Compute Between-Class Scatter Matrix (S_B)**:\n",
    "    - `S_B = (m1 - m0)(m1 - m0)^T`\n",
    "    - (Note: `S_B` itself is not directly used to find `w` in the `inv(S_W) * (m1-m0)` formulation, but it's part of Fisher's criterion J(w) = (w^T S_B w) / (w^T S_W w). The direction that maximizes this is proportional to `inv(S_W) * (m1-m0)`).\n",
    "5.  **Compute Discriminant Vector (w)**:\n",
    "    - `w = inv(S_W) @ (m1 - m0)`\n",
    "    - If `S_W` is singular, its pseudo-inverse `pinv(S_W)` is used.\n",
    "6.  **Normalize Discriminant Vector**: \n",
    "    - `w = w / ||w||` (Euclidean norm)\n",
    "    This makes the vector a unit vector, which is common practice but doesn't change its direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_discriminant(X, y):\n",
    "    \"\"\"\n",
    "    Computes Fisher's Linear Discriminant vector w.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Standardized input data (samples x features).\n",
    "        y (np.ndarray): Binary labels (0 or 1) for each sample.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized Fisher's Linear Discriminant vector w.\n",
    "                    Returns None if computation fails (e.g., singular S_W and pinv also fails, or class has no samples).\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    \n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    # Separate data by class\n",
    "    X0 = X[y == 0]\n",
    "    X1 = X[y == 1]\n",
    "\n",
    "    if X0.size == 0 or X1.size == 0:\n",
    "        print(\"Warning: One or both classes have no samples. Cannot compute Fisher Discriminant.\")\n",
    "        return None\n",
    "\n",
    "    # Compute mean vectors\n",
    "    m0 = np.mean(X0, axis=0)\n",
    "    m1 = np.mean(X1, axis=0)\n",
    "\n",
    "    # Compute within-class scatter matrices (S0 and S1)\n",
    "    s0_scatter = np.zeros((num_features, num_features))\n",
    "    for row in X0:\n",
    "        diff = (row - m0).reshape(num_features, 1)\n",
    "        s0_scatter += np.dot(diff, diff.T)\n",
    "    \n",
    "    s1_scatter = np.zeros((num_features, num_features))\n",
    "    for row in X1:\n",
    "        diff = (row - m1).reshape(num_features, 1)\n",
    "        s1_scatter += np.dot(diff, diff.T)\n",
    "\n",
    "    # Total within-class scatter matrix S_W\n",
    "    S_W = s0_scatter + s1_scatter\n",
    "\n",
    "    # Compute discriminant vector w\n",
    "    try:\n",
    "        S_W_inv = np.linalg.inv(S_W)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: S_W is singular, using pseudo-inverse.\")\n",
    "        try:\n",
    "            S_W_inv = np.linalg.pinv(S_W)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Error: Pseudo-inverse of S_W also failed. Cannot compute w.\")\n",
    "            return None\n",
    "            \n",
    "    w = S_W_inv @ (m1 - m0)\n",
    "    \n",
    "    # Normalize discriminant vector w\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    if norm_w == 0:\n",
    "        print(\"Warning: Norm of w is zero. Cannot normalize.\")\n",
    "        return w # Or None, or handle as appropriate\n",
    "    w_normalized = w / norm_w\n",
    "    \n",
    "    return w_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for fisher_discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample standardized data for Fisher's LDA\n",
    "X_exp_fisher = np.array([\n",
    "    [-1, -1], [-1.5, -0.5], [-0.5, -1.5], [-0.8, -0.8], # Class 0\n",
    "    [ 1,  1], [ 1.5,  0.5], [ 0.5,  1.5], [ 0.8,  0.8]  # Class 1\n",
    "])\n",
    "y_exp_fisher = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "# print(\"Sample X for Fisher Discriminant:\\n\", X_exp_fisher)\n",
    "# print(\"Sample y for Fisher Discriminant:\\n\", y_exp_fisher)\n",
    "\n",
    "w_fisher_example = fisher_discriminant(X_exp_fisher, y_exp_fisher)\n",
    "\n",
    "if w_fisher_example is not None:\n",
    "    pass # print(\"\\nCalculated Fisher's Discriminant vector (w):\", w_fisher_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fisher's LDA Decision Boundary Calculation\n",
    "\n",
    "Once data is projected onto the 1D space defined by Fisher's discriminant vector `w`, a decision boundary is needed to classify new points. This boundary is typically chosen as the midpoint between the means of the two classes in this projected 1D space.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Project Data**: The training data `X_train` is projected onto `w`: `X_train_lda = X_train @ w`.\n",
    "2.  **Calculate Means of Projected Classes**: \n",
    "    - `mean_class_0_lda = mean(X_train_lda[y_train == 0])`\n",
    "    - `mean_class_1_lda = mean(X_train_lda[y_train == 1])`\n",
    "3.  **Compute Decision Boundary**: \n",
    "    - `decision_boundary = (mean_class_0_lda + mean_class_1_lda) / 2.0`\n",
    "    This boundary value is a scalar in the 1D projected space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_calculation(X_train_lda, y_train):\n",
    "    \"\"\"\n",
    "    Calculates the decision boundary for 1D LDA-projected data.\n",
    "\n",
    "    Args:\n",
    "        X_train_lda (np.ndarray): Training data projected onto the LDA vector (1D array).\n",
    "        y_train (np.ndarray): Binary labels (0 or 1) for the training data.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated decision boundary.\n",
    "               Returns np.nan if one or both classes are empty in the training data.\n",
    "    \"\"\"\n",
    "    if X_train_lda.ndim != 1:\n",
    "        if X_train_lda.shape[1] == 1:\n",
    "            X_train_lda = X_train_lda.flatten()\n",
    "        else:\n",
    "            raise ValueError(\"X_train_lda must be a 1D array or a 2D array with one column.\")\n",
    "\n",
    "    class_0_projected = X_train_lda[y_train == 0]\n",
    "    class_1_projected = X_train_lda[y_train == 1]\n",
    "\n",
    "    if class_0_projected.size == 0:\n",
    "        print(\"Warning: Class 0 has no samples in X_train_lda. Cannot compute its mean.\")\n",
    "        mean_class_0_lda = np.nan\n",
    "    else:\n",
    "        mean_class_0_lda = np.mean(class_0_projected)\n",
    "        \n",
    "    if class_1_projected.size == 0:\n",
    "        print(\"Warning: Class 1 has no samples in X_train_lda. Cannot compute its mean.\")\n",
    "        mean_class_1_lda = np.nan\n",
    "    else:\n",
    "        mean_class_1_lda = np.mean(class_1_projected)\n",
    "\n",
    "    if np.isnan(mean_class_0_lda) or np.isnan(mean_class_1_lda):\n",
    "        print(\"Warning: Mean for one or both classes is NaN. Boundary calculation failed.\")\n",
    "        return np.nan\n",
    "        \n",
    "    decision_boundary = (mean_class_0_lda + mean_class_1_lda) / 2.0\n",
    "    \n",
    "    return decision_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for boundary_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'w_fisher_example' in globals() and w_fisher_example is not None:\n",
    "    if X_exp_fisher.shape[1] == len(w_fisher_example):\n",
    "        X_exp_lda = X_exp_fisher @ w_fisher_example \n",
    "        # print(\"Projected X_exp_fisher (X_exp_lda):\\n\", X_exp_lda)\n",
    "        boundary_example = boundary_calculation(X_exp_lda, y_exp_fisher)\n",
    "        # print(f\"\\nCalculated Decision Boundary: {boundary_example:.4f}\")\n",
    "    # else: print(\"Skipping boundary_calculation example due to shape mismatch.\")\n",
    "else:\n",
    "    print(\"Skipping boundary_calculation example as w_fisher_example is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LDA Classifier Function\n",
    "\n",
    "This function uses Fisher's LDA to classify test data. It first computes the discriminant vector `W` from the training data, projects both training and test data onto `W`, calculates a decision boundary from the projected training data, and then classifies the projected test data based on this boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_classifier(X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Classifies test data using Fisher's Linear Discriminant Analysis.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): Standardized training data (samples x features).\n",
    "        y_train (np.ndarray): Binary labels (0 or 1) for X_train.\n",
    "        X_test (np.ndarray): Standardized test data (samples x features).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted labels (0 or 1) for X_test. \n",
    "                    Returns an empty array if classification cannot proceed.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are 2D, this is handled by fisher_discriminant for X_train\n",
    "    # but good to ensure for X_test as well for projection.\n",
    "    if X_train.ndim == 1:\n",
    "        X_train = X_train.reshape(-1,1)\n",
    "    if X_test.ndim == 1:\n",
    "        X_test = X_test.reshape(-1,1)\n",
    "    \n",
    "    # 1. Get Discriminant Vector W\n",
    "    W = fisher_discriminant(X_train, y_train)\n",
    "    if W is None:\n",
    "        print(\"LDA Classifier Error: Could not compute discriminant vector W.\")\n",
    "        return np.array([]) # Return empty array for predictions\n",
    "\n",
    "    # 2. Project Data\n",
    "    # Ensure W is 1D for dot product with X_train (N, D) @ W (D,) -> (N,)\n",
    "    if W.ndim > 1 and W.shape[1] == 1:\n",
    "        W = W.flatten() # Convert column vector to 1D array if necessary\n",
    "    elif W.ndim > 1:\n",
    "        print(f\"LDA Classifier Error: Discriminant vector W has unexpected shape {W.shape}.\")\n",
    "        return np.array([])\n",
    "        \n",
    "    X_train_lda = X_train @ W\n",
    "    X_test_lda = X_test @ W\n",
    "\n",
    "    # 3. Calculate Decision Boundary\n",
    "    decision_boundary = boundary_calculation(X_train_lda, y_train)\n",
    "    if np.isnan(decision_boundary):\n",
    "        print(\"LDA Classifier Error: Could not compute decision boundary (resulted in NaN).\")\n",
    "        return np.array([]) # Return empty array for predictions\n",
    "\n",
    "    # 4. Classify Test Data\n",
    "    # If w = inv(Sw)(m1 - m0), then m1_projected > m0_projected (usually, if w points towards class 1)\n",
    "    # So, if X_test_lda >= decision_boundary, it's closer to mean of class 1.\n",
    "    # We need to determine which class corresponds to larger projected values.\n",
    "    # Let's check means on projected training data.\n",
    "    mean_class_0_lda = np.mean(X_train_lda[y_train == 0])\n",
    "    mean_class_1_lda = np.mean(X_train_lda[y_train == 1])\n",
    "\n",
    "    if mean_class_1_lda >= mean_class_0_lda:\n",
    "        # Class 1 has larger (or equal) projected values\n",
    "        y_pred = np.where(X_test_lda >= decision_boundary, 1, 0)\n",
    "    else:\n",
    "        # Class 0 has larger projected values (w might be pointing in the opposite direction)\n",
    "        y_pred = np.where(X_test_lda < decision_boundary, 1, 0) # Note: < for class 1 if m1 < m0\n",
    "        # Or, more consistently, predict class 0 if X_test_lda >= boundary (closer to m0_proj)\n",
    "        # y_pred = np.where(X_test_lda >= decision_boundary, 0, 1) # This line is more consistent with m0 > m1\n",
    "        # Let's stick to the rule: if projected value > boundary, it's class 1, if W was defined as m1-m0. \n",
    "        # The boundary is midpoint. The direction of W (m1-m0) implies m1_proj > m0_proj.\n",
    "        # So, X_test_lda >= decision_boundary should generally correspond to class 1.\n",
    "        # The check above handles if m1_proj < m0_proj (e.g. due to S_W effects or if w was -(m1-m0))\n",
    "        # The most robust way is to compare a test point's projection to the projected means.\n",
    "        # However, the prompt implies a simple threshold comparison.\n",
    "        # The current `np.where(X_test_lda >= decision_boundary, 1, 0)` is standard if m1_proj > m0_proj.\n",
    "        # If m0_proj > m1_proj, then this rule effectively assigns to class 0 for values > boundary.\n",
    "        # Let's make it explicit: assign to class whose projected mean is further in that direction.\n",
    "        if mean_class_1_lda > mean_class_0_lda:\n",
    "            y_pred = np.where(X_test_lda >= decision_boundary, 1, 0)\n",
    "        else: # mean_class_0_lda > mean_class_1_lda (or equal, covered by first case)\n",
    "            y_pred = np.where(X_test_lda >= decision_boundary, 0, 1) # If value > boundary, it's closer to m0\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for lda_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using X_exp_fisher and y_exp_fisher from the fisher_discriminant example as training data\n",
    "X_train_lda_example = X_exp_fisher\n",
    "y_train_lda_example = y_exp_fisher\n",
    "\n",
    "# Create sample test data (standardized)\n",
    "X_test_lda_example = np.array([\n",
    "    [-2, -2],   # Expected class 0\n",
    "    [ 2,  2],   # Expected class 1\n",
    "    [ 0,  0],   # Expected boundary case, depends on tie-breaking or exact boundary\n",
    "    [-0.5, 1],  # Mixed, depends on projection\n",
    "    [ 1, -0.5]   # Mixed, depends on projection\n",
    "])\n",
    "\n",
    "print(\"LDA Classifier Example:\")\n",
    "print(\"X_train:\\n\", X_train_lda_example)\n",
    "print(\"y_train:\\n\", y_train_lda_example)\n",
    "print(\"X_test:\\n\", X_test_lda_example)\n",
    "\n",
    "y_predictions_lda = lda_classifier(X_train_lda_example, y_train_lda_example, X_test_lda_example)\n",
    "\n",
    "if y_predictions_lda is not None and y_predictions_lda.size > 0:\n",
    "    print(\"\\nPredicted labels for X_test:\", y_predictions_lda)\n",
    "    # For X_exp_fisher, w_fisher_example pointed towards class 1 (larger projected values for class 1).\n",
    "    # Boundary was near 0. \n",
    "    # Test point [-2,-2] projected by w ~[0.7,0.7] is ~ -2.8 -> class 0\n",
    "    # Test point [2,2] projected by w ~[0.7,0.7] is ~ 2.8 -> class 1\n",
    "    # Test point [0,0] projected is 0 -> class 1 (if boundary is ~0 and rule is >= boundary -> 1)\n",
    "    # Expected (approximate): [0, 1, 1, ?, ?] - last two depend on projection relative to boundary\n",
    "else:\n",
    "    print(\"LDA classification failed or returned no predictions.\")\n",
    "\n",
    "# Example with 1D data\n",
    "X_train_1d_lda = np.array([-1, -2, -0.5, 1, 2, 0.5]).reshape(-1, 1)\n",
    "y_train_1d_lda = np.array([0, 0, 0, 1, 1, 1])\n",
    "X_test_1d_lda = np.array([-3, 0, 3]).reshape(-1,1)\n",
    "\n",
    "print(\"\\nLDA Classifier Example (1D data):\")\n",
    "y_preds_1d_lda = lda_classifier(X_train_1d_lda, y_train_1d_lda, X_test_1d_lda)\n",
    "if y_preds_1d_lda is not None and y_preds_1d_lda.size > 0:\n",
    "    print(\"Predicted labels for 1D X_test:\", y_preds_1d_lda)\n",
    "    # m0 ~ -1.16, m1 ~ 1.16. Boundary ~0. W for 1D is [1] or [-1]. Let's say W=[1].\n",
    "    # X_test_lda = [-3,0,3]\n",
    "    # Predictions: [0,1,1] (if boundary=0, rule is >=0 -> 1)\n",
    "else:\n",
    "    print(\"LDA classification for 1D data failed or returned no predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
