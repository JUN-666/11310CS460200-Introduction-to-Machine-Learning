{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Decision Trees and Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json # For pretty printing the dictionary tree structure\n",
    "from sklearn.metrics import f1_score # For F1 score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Core Decision Tree Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Entropy Function\n",
    "\n",
    "This function calculates the entropy of a given dataset based on the target column 'hospital_death'.\n",
    "Entropy is a measure of impurity or disorder in a dataset.\n",
    "\n",
    "**Formula:** E = - sum(p_i * log2(p_i)) for all classes i.\n",
    "\n",
    "For a binary classification problem with target 'hospital_death' (classes 0 and 1):\n",
    "E = - (p_0 * log2(p_0) + p_1 * log2(p_1))\n",
    "where p_0 is the proportion of class 0 (e.g., not survived) and p_1 is the proportion of class 1 (e.g., survived).\n",
    "\n",
    "If a proportion p_i is 0, then p_i * log2(p_i) is taken as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a dataset based on the 'hospital_death' target column.\n",
    "    \"\"\"\n",
    "    target_column = 'hospital_death'\n",
    "    if data.empty or target_column not in data.columns:\n",
    "        return 0.0\n",
    "    counts = data[target_column].value_counts()\n",
    "    total_count = len(data[target_column])\n",
    "    if total_count == 0 or len(counts) <= 1:\n",
    "        return 0.0\n",
    "    entropy_value = 0.0\n",
    "    for count_val in counts:\n",
    "        proportion = count_val / total_count\n",
    "        if proportion > 0:\n",
    "            entropy_value -= proportion * np.log2(proportion)\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage for Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_for_lab2 = {\n",
    "    'age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
    "    'gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female'], \n",
    "    'bmi': [22.5, 24.1, 26.3, 28.0, 29.5, 30.2, 25.5, 27.8], \n",
    "    'hospital_death': [0, 1, 0, 0, 1, 1, 0, 1]\n",
    "}\n",
    "sample_df_lab2 = pd.DataFrame(sample_data_for_lab2)\n",
    "# print(f\"Sample DataFrame for Lab 2:\\n{sample_df_lab2}\")\n",
    "# print(f\"\\nCalculated Entropy for sample_df_lab2: {entropy(sample_df_lab2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Information Gain Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data, mask):\n",
    "    if data.empty: return 0.0\n",
    "    parent_entropy = entropy(data)\n",
    "    left_subset = data[mask]\n",
    "    right_subset = data[~mask]\n",
    "    total_count = len(data)\n",
    "    if total_count == 0: return 0.0\n",
    "    entropy_left = entropy(left_subset)\n",
    "    entropy_right = entropy(right_subset)\n",
    "    weight_left = len(left_subset) / total_count if total_count > 0 else 0\n",
    "    weight_right = len(right_subset) / total_count if total_count > 0 else 0\n",
    "    weighted_children_entropy = (weight_left * entropy_left) + (weight_right * entropy_right)\n",
    "    return parent_entropy - weighted_children_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Find Best Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(data, impl_part='basic'):\n",
    "    best_ig = -1.0\n",
    "    best_threshold = None\n",
    "    best_feature = None\n",
    "    if data.empty or 'hospital_death' not in data.columns or len(data) < 2:\n",
    "        return best_ig, best_threshold, best_feature\n",
    "    features_to_consider = data.columns.drop('hospital_death', errors='ignore')\n",
    "    for feature in features_to_consider:\n",
    "        if pd.api.types.is_numeric_dtype(data[feature]):\n",
    "            unique_values = sorted(data[feature].unique())\n",
    "            if len(unique_values) < 2: continue\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                val1, val2 = unique_values[i], unique_values[i+1]\n",
    "                if val1 == val2: continue\n",
    "                threshold = (val1 + val2) / 2.0\n",
    "                mask = data[feature] <= threshold\n",
    "                if mask.all() or (~mask).all(): continue\n",
    "                current_ig = information_gain(data, mask)\n",
    "                if current_ig > best_ig:\n",
    "                    best_ig, best_threshold, best_feature = current_ig, threshold, feature\n",
    "    return best_ig, best_threshold, best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Make Partition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_partition(data, feature, threshold):\n",
    "    if feature not in data.columns: return pd.DataFrame(), pd.DataFrame()\n",
    "    left_df = data[data[feature] <= threshold]\n",
    "    right_df = data[data[feature] > threshold]\n",
    "    return left_df, right_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Build Tree Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_features = [] \n",
    "ans_thresholds = [] \n",
    "\n",
    "def build_tree(data, max_depth, min_samples_split, current_depth, impl_part='basic'):\n",
    "    if data.empty or 'hospital_death' not in data.columns: return 0\n",
    "    majority_class = data['hospital_death'].mode()\n",
    "    leaf_value = majority_class[0] if not majority_class.empty else 0\n",
    "    if (current_depth >= max_depth or len(data) < min_samples_split or len(data['hospital_death'].unique()) == 1):\n",
    "        return leaf_value\n",
    "    best_ig, best_threshold, best_feature = find_best_split(data, impl_part)\n",
    "    if best_ig <= 0 or best_feature is None: return leaf_value\n",
    "    left_data, right_data = make_partition(data, best_feature, best_threshold)\n",
    "    if left_data.empty or right_data.empty: return leaf_value\n",
    "    if impl_part == 'basic':\n",
    "        ans_features.append(best_feature)\n",
    "        ans_thresholds.append(best_threshold)\n",
    "    next_depth = current_depth + 1\n",
    "    left_subtree = build_tree(left_data, max_depth, min_samples_split, next_depth, impl_part)\n",
    "    right_subtree = build_tree(right_data, max_depth, min_samples_split, next_depth, impl_part)\n",
    "    question = f'{best_feature} <= {best_threshold:.4f}'\n",
    "    return {question: [left_subtree, right_subtree]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Classify Data and Make Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(instance, tree):\n",
    "    if not isinstance(tree, dict): return tree\n",
    "    question = list(tree.keys())[0]\n",
    "    feature_name, comparison_operator, value_str = question.split(' ', 2)\n",
    "    if feature_name not in instance: raise ValueError(f\"Feature '{feature_name}' not in instance.\")\n",
    "    instance_value = instance[feature_name]\n",
    "    threshold = float(value_str)\n",
    "    answer = tree[question][0] if instance_value <= threshold else tree[question][1]\n",
    "    return classify_data(instance, answer)\n",
    "\n",
    "def make_prediction(tree, data):\n",
    "    if data.empty: return []\n",
    "    return data.apply(classify_data, axis=1, args=(tree,)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train and Evaluate Decision Tree (Basic Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is for the basic decision tree training and evaluation.\n",
    "# It uses lab2_basic_input.csv and outputs ans_features, ans_thresholds, y_pred, ans_f1score.\n",
    "# For brevity, example outputs from this section are not repeatedly printed in later advanced sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    input_data_basic = pd.read_csv('lab2_basic_input.csv')\n",
    "except FileNotFoundError:\n",
    "    data_dict_basic = {\n",
    "        'age': np.random.randint(20, 80, 40),'bmi': np.random.uniform(18.5, 40.0, 40).round(2),\n",
    "        'glucose_apache': np.random.randint(70, 250, 40),'hospital_death': np.random.randint(0, 2, 40)\n",
    "    }\n",
    "    input_data_basic = pd.DataFrame(data_dict_basic)\n",
    "    input_data_basic.loc[0:1, 'hospital_death'] = 0; input_data_basic.loc[2:3, 'hospital_death'] = 1\n",
    "    input_data_basic.loc[0, 'age'] = 30; input_data_basic.loc[1, 'age'] = 60\n",
    "    input_data_basic.loc[0, 'bmi'] = 22.0; input_data_basic.loc[1, 'bmi'] = 30.0\n",
    "# print(\"Basic Input Data Head:\\n\", input_data_basic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_basic = 30; num_validation_basic = 10\n",
    "if len(input_data_basic) < num_train_basic + num_validation_basic:\n",
    "    if len(input_data_basic) < num_train_basic: num_train_basic = len(input_data_basic); num_validation_basic = 0\n",
    "    else: num_validation_basic = len(input_data_basic) - num_train_basic\n",
    "training_data_basic = input_data_basic.iloc[:num_train_basic]\n",
    "validation_data_basic = input_data_basic.iloc[num_train_basic:num_train_basic + num_validation_basic]\n",
    "x_validation_basic = validation_data_basic.drop(['hospital_death'], axis=1, errors='ignore')\n",
    "y_validation_flat_basic = validation_data_basic['hospital_death'].values.flatten() if 'hospital_death' in validation_data_basic else np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set Tree Parameters and Initialize Tracking Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_basic = 2; min_samples_split_basic = 2; depth_basic = 0\n",
    "ans_features = []; ans_thresholds = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_basic = None\n",
    "if not training_data_basic.empty and 'hospital_death' in training_data_basic.columns:\n",
    "    decision_tree_basic = build_tree(training_data_basic, max_depth_basic, min_samples_split_basic, depth_basic, 'basic')\n",
    "    # print(\"Basic Decision Tree Structure:\", json.dumps(decision_tree_basic, indent=2))\n",
    "    # print(f\"Features used (Basic): {ans_features}\")\n",
    "    # print(f\"Thresholds used (Basic): {ans_thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Make Predictions on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_basic = []\n",
    "if decision_tree_basic is not None and not x_validation_basic.empty and decision_tree_basic:\n",
    "    y_pred_basic = make_prediction(decision_tree_basic, x_validation_basic)\n",
    "    # print(f\"Predictions (y_pred_basic) on basic validation data: {y_pred_basic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(y_true, y_pred):\n",
    "    if not isinstance(y_true, (list, np.ndarray)) or not isinstance(y_pred, (list, np.ndarray)): return 0.0\n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_true) != len(y_pred): return 0.0\n",
    "    return f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "ans_f1score_basic = 0.0\n",
    "if len(y_validation_flat_basic) > 0 and len(y_pred_basic) == len(y_validation_flat_basic):\n",
    "    ans_f1score_basic = calculate_score(y_validation_flat_basic, y_pred_basic)\n",
    "    ans_f1score_basic = round(ans_f1score_basic, 4)\n",
    "    # print(f\"F1 Score on Basic Validation Data: {ans_f1score_basic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Advanced Random Forest (Optional Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Build Forest Function\n",
    "\n",
    "Builds a random forest with bootstrapped data and feature subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_forest(data, n_trees, n_features_per_tree, n_samples_per_tree, max_depth, min_samples_split):\n",
    "    forest = []\n",
    "    all_feature_names = data.columns.drop('hospital_death', errors='ignore').tolist()\n",
    "    if not all_feature_names: return forest\n",
    "    if n_features_per_tree > len(all_feature_names): n_features_per_tree = len(all_feature_names)\n",
    "\n",
    "    for i in range(n_trees):\n",
    "        # print(f\"\\nBuilding tree {i + 1}/{n_trees}...\")\n",
    "        selected_row_indices = np.random.choice(data.index, size=n_samples_per_tree, replace=True)\n",
    "        tree_data_sampled_rows = data.loc[selected_row_indices]\n",
    "        # print(f\"  Selected row indices (first 10): {selected_row_indices.tolist()[:10]}...\")\n",
    "        selected_features = np.random.choice(all_feature_names, size=n_features_per_tree, replace=False).tolist()\n",
    "        # print(f\"  Selected features: {selected_features}\")\n",
    "        tree_data_final = tree_data_sampled_rows[selected_features + ['hospital_death']]\n",
    "        tree = build_tree(tree_data_final, max_depth, min_samples_split, 0, impl_part='advanced')\n",
    "        # if isinstance(tree, dict): print(f\"    Root node: {list(tree.keys())[0]} ...\") \n",
    "        # else: print(f\"    Root node: Leaf value = {tree}\")\n",
    "        forest.append(tree)\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage for build_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- Example for build_forest ---\")\n",
    "n_trees_ex_forest = 1 # Small number for quick example run\n",
    "max_depth_ex_forest = 2\n",
    "min_samples_ex_forest = 2\n",
    "all_feat_count_ex = len(sample_df_lab2.columns.drop('hospital_death', errors='ignore'))\n",
    "n_feat_tree_ex = max(1, all_feat_count_ex // 2) if all_feat_count_ex > 0 else 0\n",
    "n_samp_tree_ex = len(sample_df_lab2) // 2 if len(sample_df_lab2) // 2 > 0 else 1\n",
    "\n",
    "example_forest_run = []\n",
    "if n_feat_tree_ex > 0 and n_samp_tree_ex > 0 and not sample_df_lab2.empty:\n",
    "    example_forest_run = build_forest(sample_df_lab2, n_trees_ex_forest, n_feat_tree_ex, n_samp_tree_ex, max_depth_ex_forest, min_samples_ex_forest)\n",
    "    # print(f\"\\nNumber of trees in example_forest_run: {len(example_forest_run)}\")\n",
    "    # if example_forest_run and isinstance(example_forest_run[0], dict): print(json.dumps(example_forest_run[0], indent=2))\n",
    "    # elif example_forest_run: print(f\"First tree is a leaf: {example_forest_run[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Make Prediction with Forest Function\n",
    "\n",
    "Aggregates predictions from all trees in a forest by majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_forest(forest, data):\n",
    "    if not forest: return []\n",
    "    if data.empty: return []\n",
    "    all_tree_predictions = []\n",
    "    for i, tree in enumerate(forest):\n",
    "        if tree is None or (not isinstance(tree, dict) and not isinstance(tree, (int, float, np.number))):\n",
    "            # print(f\"Warning: Tree {i} is invalid. Skipping.\") # Potentially add default predictions for this tree for all instances\n",
    "            num_instances = len(data)\n",
    "            all_tree_predictions.append([0] * num_instances) # Defaulting to predict 0 for this tree's predictions\n",
    "            continue \n",
    "        current_tree_predictions = make_prediction(tree, data)\n",
    "        all_tree_predictions.append(current_tree_predictions)\n",
    "\n",
    "    if not all_tree_predictions: return [0] * len(data)\n",
    "    try:\n",
    "        predictions_array = np.array(all_tree_predictions)\n",
    "    except ValueError:\n",
    "        # This might happen if make_prediction returns variable length lists (should not with current code)\n",
    "        # Or if a tree was skipped and not handled by adding a placeholder list of predictions\n",
    "        # Fallback: use predictions from the first valid tree, or default to 0\n",
    "        for preds in all_tree_predictions: # Find first non-empty list\n",
    "            if preds and len(preds) == len(data): return preds\n",
    "        return [0] * len(data) \n",
    "\n",
    "    transposed_predictions = predictions_array.T\n",
    "    final_predictions = []\n",
    "    for instance_predictions in transposed_predictions:\n",
    "        count_0 = np.count_nonzero(instance_predictions == 0)\n",
    "        count_1 = np.count_nonzero(instance_predictions == 1)\n",
    "        majority = 1 if count_1 >= count_0 else 0\n",
    "        final_predictions.append(majority)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage for make_prediction_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- Example for make_prediction_forest ---\")\n",
    "if 'example_forest_run' in globals() and isinstance(example_forest_run, list) and example_forest_run and \\\n",
    "   'sample_df_lab2' in globals() and not sample_df_lab2.empty:\n",
    "    forest_pred_data_ex = sample_df_lab2.drop(columns=['hospital_death'], errors='ignore')\n",
    "    forest_preds_ex = make_prediction_forest(example_forest_run, forest_pred_data_ex)\n",
    "    # print(f\"Forest Predictions on sample_df_lab2: {forest_preds_ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Train and Evaluate Random Forest (Advanced Part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Advanced Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    advanced_training_data = pd.read_csv('lab2_advanced_training.csv')\n",
    "    advanced_testing_data = pd.read_csv('lab2_advanced_testing.csv')\n",
    "    print(\"Successfully loaded advanced training and testing data.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Advanced data files not found. Creating dummy advanced data.\")\n",
    "    adv_train_dict = {\n",
    "        'feature1': np.random.rand(100) * 10, 'feature2': np.random.rand(100) * 5,\n",
    "        'feature3': np.random.randint(0, 5, 100), 'feature4': np.random.normal(50, 10, 100),\n",
    "        'feature5': np.random.uniform(1, 10, 100),\n",
    "        'hospital_death': np.random.randint(0, 2, 100)\n",
    "    }\n",
    "    advanced_training_data = pd.DataFrame(adv_train_dict)\n",
    "    adv_test_dict = {\n",
    "        'feature1': np.random.rand(50) * 10, 'feature2': np.random.rand(50) * 5,\n",
    "        'feature3': np.random.randint(0, 5, 50), 'feature4': np.random.normal(50, 10, 50),\n",
    "        'feature5': np.random.uniform(1, 10, 50)\n",
    "    }\n",
    "    advanced_testing_data = pd.DataFrame(adv_test_dict)\n",
    "    print(\"Created dummy advanced training and testing data.\")\n",
    "\n",
    "print(f\"Advanced training data shape: {advanced_training_data.shape}\")\n",
    "print(f\"Advanced testing data shape: {advanced_testing_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Training Data for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_adv = 0.8\n",
    "adv_train_df = advanced_training_data.sample(frac=split_ratio_adv, random_state=42)\n",
    "adv_val_df = advanced_training_data.drop(adv_train_df.index)\n",
    "\n",
    "x_adv_train = adv_train_df.drop(columns=['hospital_death'], errors='ignore')\n",
    "y_adv_train = adv_train_df['hospital_death'] if 'hospital_death' in adv_train_df else pd.Series()\n",
    "\n",
    "x_adv_val = adv_val_df.drop(columns=['hospital_death'], errors='ignore')\n",
    "y_adv_val = adv_val_df['hospital_death'] if 'hospital_death' in adv_val_df else pd.Series()\n",
    "y_adv_val_flat = y_adv_val.values.flatten()\n",
    "\n",
    "print(f\"Advanced training features shape: {x_adv_train.shape}\")\n",
    "print(f\"Advanced validation features shape: {x_adv_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set Random Forest Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_rf = 5\n",
    "min_samples_split_rf = 2\n",
    "n_trees = 5 # Using a small number for quick example run\n",
    "\n",
    "n_features_all_adv = x_adv_train.shape[1]\n",
    "if n_features_all_adv > 0:\n",
    "    n_features_per_tree = int(np.sqrt(n_features_all_adv))\n",
    "    n_features_per_tree = max(1, n_features_per_tree) # Ensure at least 1 feature\n",
    "else:\n",
    "    n_features_per_tree = 0 # No features available\n",
    "\n",
    "n_samples_per_tree = len(adv_train_df) # Full bootstrap sample size\n",
    "if n_samples_per_tree == 0 and len(advanced_training_data) > 0: # Handle case where adv_train_df might be empty if split_ratio is 0\n    n_samples_per_tree = len(advanced_training_data)\n\n",
    "print(f\"Random Forest Parameters: max_depth={max_depth_rf}, min_samples_split={min_samples_split_rf}, n_trees={n_trees}\")\n",
    "print(f\"Features per tree: {n_features_per_tree}, Samples per tree: {n_samples_per_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_forest = [] # Initialize\n",
    "if not adv_train_df.empty and 'hospital_death' in adv_train_df.columns and n_features_per_tree > 0:\n",
    "    adv_forest = build_forest(adv_train_df, n_trees, n_features_per_tree, n_samples_per_tree, max_depth_rf, min_samples_split_rf)\n",
    "    print(f\"\\nRandom Forest built with {len(adv_forest)} trees.\")\n",
    "else:\n",
    "    print(\"Cannot build forest: Training data is empty, 'hospital_death' is missing, or no features to select for trees.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validate Random Forest on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_val_rf = 0.0\n",
    "if adv_forest and not x_adv_val.empty and y_adv_val_flat.size > 0:\n",
    "    y_pred_val_rf = make_prediction_forest(adv_forest, x_adv_val)\n",
    "    f1_score_val_rf = calculate_score(y_adv_val_flat, y_pred_val_rf)\n",
    "    f1_score_val_rf = round(f1_score_val_rf, 4)\n",
    "    print(f\"F1 Score of Random Forest on Advanced Validation Data: {f1_score_val_rf}\")\n",
    "elif not adv_forest:\n",
    "    print(\"Forest not built. Skipping validation.\")\n",
    "else:\n",
    "    print(\"Validation data is empty or has no labels. Skipping F1 score calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced = [] # This list will store the final predictions for the test set\n",
    "if adv_forest and not advanced_testing_data.empty:\n",
    "    y_pred_test_rf = make_prediction_forest(adv_forest, advanced_testing_data)\n",
    "    advanced = y_pred_test_rf # Store predictions in the 'advanced' list\n",
    "    print(f\"Predictions made on advanced testing data. First 10 predictions: {advanced[:10]}\")\n",
    "elif not adv_forest:\n",
    "    print(\"Forest not built. Skipping predictions on test data.\")\n",
    "else:\n",
    "    print(\"Advanced testing data is empty. No predictions made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write Output File for Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'advanced' in globals() and isinstance(advanced, list) and advanced:\n",
    "    if len(advanced) == len(advanced_testing_data):\n",
    "        # Assuming advanced_testing_data does not have an 'Id' column, so create sequential IDs\n",
    "        output_df_adv = pd.DataFrame({\n",
    "            'Id': np.arange(1, len(advanced) + 1),\n",
    "            'hospital_death': advanced\n",
    "        })\n",
    "        output_df_adv.to_csv('lab2_advanced.csv', index=False)\n",
    "        print(f\"Advanced predictions saved to lab2_advanced.csv. Total predictions: {len(advanced)}\")\n",
    "    else:\n",
    "        print(f\"Error: Length of predictions ({len(advanced)}) does not match test data ({len(advanced_testing_data)}). Output file not saved.\")\n",
    "elif 'advanced' in globals() and isinstance(advanced, list) and not advanced and not advanced_testing_data.empty:\n",
    "    print(\"No predictions in 'advanced' list, but test data exists. Output file not saved.\")\n",
    "else:\n",
    "    print(\"Error: 'advanced' predictions list not found or empty, or test data is empty. Cannot save output file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
