{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Splitting, Preprocessing, and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SplitData Function\n",
    "\n",
    "This function splits a NumPy array into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(data, split_ratio):\n",
    "  \"\"\"\n",
    "  Splits a NumPy array into training and validation sets.\n",
    "\n",
    "  Args:\n",
    "    data: A numpy.ndarray to be split.\n",
    "    split_ratio: The ratio for splitting the data (e.g., 0.8 for 80% training).\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing two numpy.ndarrays: (training_data, validation_data).\n",
    "  \"\"\"\n",
    "  # Ensure data is a NumPy array\n",
    "  if not isinstance(data, np.ndarray):\n",
    "    data = np.array(data)\n",
    "  split_index = int(len(data) * split_ratio)\n",
    "  training_data = data[:split_index]\n",
    "  validation_data = data[split_index:]\n",
    "  return training_data, validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for SplitData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset for SplitData\n",
    "sample_data_split = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]])\n",
    "split_ratio_val = 0.8\n",
    "\n",
    "# Split the data\n",
    "training_set, validation_set = SplitData(sample_data_split, split_ratio_val)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Data for SplitData:\\n\", sample_data_split)\n",
    "print(\"Training Data:\\n\", training_set)\n",
    "print(\"Validation Data:\\n\", validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreprocessData Function\n",
    "\n",
    "This function preprocesses a NumPy array by handling missing values and outliers for numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessData(data):\n",
    "  \"\"\"\n",
    "  Preprocesses a NumPy array by removing rows with NaN values and removing outliers based on IQR.\n",
    "\n",
    "  Args:\n",
    "    data: A numpy.ndarray to be preprocessed. Assumes numerical data.\n",
    "          Can be 1D (single feature) or 2D (multiple features or feature and target).\n",
    "\n",
    "  Returns:\n",
    "    A numpy.ndarray with missing values and outliers removed.\n",
    "    If input was 1D, output is 2D with one column.\n",
    "  \"\"\"\n",
    "  if not isinstance(data, np.ndarray):\n",
    "    data = np.array(data, dtype=float)\n",
    "  else:\n",
    "    data = data.astype(float)\n",
    "\n",
    "  if data.ndim == 1:\n",
    "    data = data.reshape(-1, 1)\n",
    "\n",
    "  data_no_nan = data[~np.isnan(data).any(axis=1)]\n",
    "  \n",
    "  if data_no_nan.size == 0:\n",
    "      print(\"Warning: All rows removed after NaN handling or data was initially empty.\")\n",
    "      return data_no_nan\n",
    "\n",
    "  Q1 = np.percentile(data_no_nan, 25, axis=0)\n",
    "  Q3 = np.percentile(data_no_nan, 75, axis=0)\n",
    "  IQR = Q3 - Q1\n",
    "  \n",
    "  lower_bound = Q1 - 1.5 * IQR\n",
    "  upper_bound = Q3 + 1.5 * IQR\n",
    "  \n",
    "  non_outlier_mask = np.all((data_no_nan >= lower_bound) & (data_no_nan <= upper_bound), axis=1)\n",
    "  preprocessed_data = data_no_nan[non_outlier_mask]\n",
    "  \n",
    "  if preprocessed_data.size == 0 and data_no_nan.size > 0:\n",
    "      print(\"Warning: All rows removed after outlier handling.\")\n",
    "      \n",
    "  return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for PreprocessData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_preprocess = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, np.nan, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 100],\n",
    "    [13, 14, 15],\n",
    "    [-50, 16, 17]\n",
    "], dtype=float)\n",
    "print(\"Original Data for Preprocessing:\\n\", sample_data_preprocess)\n",
    "preprocessed_set = PreprocessData(sample_data_preprocess)\n",
    "print(\"Preprocessed Data (2D):\\n\", preprocessed_set)\n",
    "\n",
    "sample_data_1d = np.array([1, 2, 3, 4, 5, 100, np.nan])\n",
    "print(\"\\nOriginal 1D Data for Preprocessing:\\n\", sample_data_1d)\n",
    "preprocessed_set_1d = PreprocessData(sample_data_1d)\n",
    "print(\"Preprocessed 1D Data (becomes 2D):\\n\", preprocessed_set_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Function (Polynomial with Gradient Descent)\n",
    "\n",
    "This function performs polynomial regression using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression(dataset, degree=2, num_iteration=1000, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Performs polynomial regression using gradient descent.\n",
    "    Assumes dataset's first column is feature X, second is target y.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset, np.ndarray):\n",
    "        dataset = np.array(dataset, dtype=float)\n",
    "    if dataset.ndim == 1 or dataset.shape[1] < 2:\n",
    "        raise ValueError(\"Dataset for regression must be 2D with at least two columns (features and target).\")\n",
    "    \n",
    "    X_feature = dataset[:, 0]\n",
    "    y = dataset[:, 1]\n",
    "    m = len(y)\n",
    "\n",
    "    X_poly = np.ones((m, 1))\n",
    "    for p in range(1, degree + 1):\n",
    "        X_p = X_feature**p\n",
    "        X_poly = np.concatenate((X_poly, X_p.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    w = np.zeros(degree + 1)\n",
    "    for i in range(num_iteration):\n",
    "        y_pred = X_poly @ w\n",
    "        cost = np.mean((y_pred - y)**2)\n",
    "        gradient = (2/m) * X_poly.T @ (y_pred - y)\n",
    "        w = w - learning_rate * gradient\n",
    "        if num_iteration > 10 and ((i + 1) % (num_iteration // 10) == 0 or i == num_iteration -1):\n",
    "            print(f\"Iteration {i+1}/{num_iteration}, Cost: {cost:.4f}\")\n",
    "        elif num_iteration <= 10:\n",
    "             print(f\"Iteration {i+1}/{num_iteration}, Cost: {cost:.4f}\")\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_sample_reg = np.sort(np.random.rand(50) * 10 - 5)\n",
    "y_sample_reg = 0.5 * X_sample_reg**2 + X_sample_reg + 2 + np.random.randn(50) * 5 \n",
    "sample_reg_dataset = np.vstack((X_sample_reg, y_sample_reg)).T\n",
    "print(\"Sample Regression Dataset shape:\", sample_reg_dataset.shape)\n",
    "degree_val_ex = 2\n",
    "iterations_val_ex = 10000\n",
    "lr_val_ex = 0.001 \n",
    "learned_weights_ex = Regression(sample_reg_dataset, degree=degree_val_ex, num_iteration=iterations_val_ex, learning_rate=lr_val_ex)\n",
    "print(\"\\nLearned Weights (example):\")\n",
    "print(learned_weights_ex)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_sample_reg, y_sample_reg, label='Original Data', color='blue', alpha=0.7)\n",
    "X_plot_ex = np.linspace(min(X_sample_reg), max(X_sample_reg), 100)\n",
    "X_plot_poly_ex = np.ones((len(X_plot_ex), 1))\n",
    "for p_ex in range(1, degree_val_ex + 1):\n",
    "    X_p_plot_ex = X_plot_ex**p_ex\n",
    "    X_plot_poly_ex = np.concatenate((X_plot_poly_ex, X_p_plot_ex.reshape(-1, 1)), axis=1)\n",
    "y_plot_ex = X_plot_poly_ex @ learned_weights_ex\n",
    "plt.plot(X_plot_ex, y_plot_ex, label=f'Fitted Polynomial (Degree {degree_val_ex})', color='red', linewidth=2)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Regression with Gradient Descent Example')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MakePrediction Function (for Polynomial Regression)\n",
    "\n",
    "Uses learned weights from `Regression` to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePrediction(w, test_features):\n",
    "    \"\"\"\n",
    "    Makes predictions on test data using learned polynomial regression weights.\n",
    "    Assumes test_features is a 1D array of the single feature X.\n",
    "    \"\"\"\n",
    "    if not isinstance(w, np.ndarray):\n",
    "        w = np.array(w)\n",
    "    if not isinstance(test_features, np.ndarray):\n",
    "        test_features = np.array(test_features, dtype=float)\n",
    "    elif test_features.ndim == 0:\n",
    "        test_features = np.array([test_features], dtype=float)\n",
    "    \n",
    "    degree = len(w) - 1\n",
    "    m_test = len(test_features)\n",
    "\n",
    "    X_poly_test = np.ones((m_test, 1))\n",
    "    for p in range(1, degree + 1):\n",
    "        X_p_test = test_features**p\n",
    "        X_poly_test = np.concatenate((X_poly_test, X_p_test.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    predictions = X_poly_test @ w\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage for MakePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'learned_weights_ex' in globals() and 'X_sample_reg' in globals() and 'y_sample_reg' in globals():\n",
    "    test_data_points_ex = np.array([-4, 0, 4])\n",
    "    predictions_on_test_ex = MakePrediction(learned_weights_ex, test_data_points_ex)\n",
    "    print(f\"Test Data Points (example): {test_data_points_ex}\")\n",
    "    print(f\"Predictions on Test Data (example): {predictions_on_test_ex}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_sample_reg, y_sample_reg, label='Original Data', color='blue', alpha=0.7) \n",
    "    if 'X_plot_ex' in globals() and 'y_plot_ex' in globals():\n",
    "        plt.plot(X_plot_ex, y_plot_ex, label=f'Fitted Polynomial (Degree {len(learned_weights_ex)-1})', color='red', linewidth=2)\n",
    "    plt.scatter(test_data_points_ex, predictions_on_test_ex, label='New Predictions', color='green', s=100, marker='x', zorder=5)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Polynomial Regression Example: Fitted Curve and New Predictions')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Please run the Regression example cell first to generate 'learned_weights_ex'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Generate Result (Basic Part)\n",
    "\n",
    "This section orchestrates the loading of data, preprocessing, model training, prediction, and result generation for the basic part of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_basic = 'lab1_basic_training.csv'\n",
    "testing_file_basic = 'lab1_basic_testing.csv'\n",
    "output_dataroot_basic = 'lab1_basic.csv'\n",
    "\n",
    "try:\n",
    "    training_datalist_basic = np.genfromtxt(training_file_basic, delimiter=',', skip_header=1)\n",
    "    raw_testing_datalist_basic = np.genfromtxt(testing_file_basic, delimiter=',', skip_header=1)\n",
    "    if raw_testing_datalist_basic.ndim > 1 and raw_testing_datalist_basic.shape[1] > 0:\n",
    "        testing_datalist_basic_features = raw_testing_datalist_basic[:, 0]\n",
    "    else:\n",
    "        testing_datalist_basic_features = raw_testing_datalist_basic\n",
    "    print(f\"Successfully loaded {training_file_basic} and {testing_file_basic}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Basic data files not found. Using dummy data for basic part.\")\n",
    "    training_datalist_basic = np.array([[1,2.5],[2,3.5],[3,4.8],[4,6.1],[5,7.0],[6,8.2],[7,9.5],[8,10.3],[9,11.8],[10,13.2]])\n",
    "    testing_datalist_basic_features = np.array([11, 12, 13.5, 14.2, 15.0])\n",
    "\n",
    "print(\"Initial basic training_datalist shape:\", training_datalist_basic.shape)\n",
    "print(\"Initial basic testing_datalist features shape:\", testing_datalist_basic_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Data (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_basic = 0.8\n",
    "train_data_basic, val_data_basic = SplitData(training_datalist_basic, split_ratio_basic)\n",
    "print(f\"Basic training data shape: {train_data_basic.shape}\")\n",
    "print(f\"Basic validation data shape: {val_data_basic.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess Data (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_data_basic = PreprocessData(train_data_basic)\n",
    "print(f\"Processed basic training data shape: {processed_train_data_basic.shape}\")\n",
    "if processed_train_data_basic.size == 0:\n",
    "    raise ValueError(\"Basic training data is empty after preprocessing.\")\n",
    "\n",
    "processed_val_data_basic = PreprocessData(val_data_basic)\n",
    "print(f\"Processed basic validation data shape: {processed_val_data_basic.shape}\")\n",
    "\n",
    "processed_testing_datalist_basic_features = PreprocessData(testing_datalist_basic_features)\n",
    "print(f\"Processed basic testing features shape: {processed_testing_datalist_basic_features.shape}\")\n",
    "if processed_testing_datalist_basic_features.size == 0:\n",
    "    raise ValueError(\"Basic testing data is empty after preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Regression Model (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_basic_train = 2 \n",
    "iterations_basic_train = 10000 \n",
    "lr_basic_train = 0.001\n",
    "print(\"Training basic regression model...\")\n",
    "w_basic = Regression(processed_train_data_basic, degree=degree_basic_train, num_iteration=iterations_basic_train, learning_rate=lr_basic_train)\n",
    "print(\"Learned weights for basic model (w_basic):\")\n",
    "print(w_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict on Validation Set and Calculate MAPE (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processed_val_data_basic.size > 0 and processed_val_data_basic.shape[1] >= 2:\n",
    "    val_features_basic = processed_val_data_basic[:, 0]\n",
    "    val_labels_basic = processed_val_data_basic[:, 1]\n",
    "    val_predictions_basic = MakePrediction(w_basic, val_features_basic)\n",
    "\n",
    "    try:\n",
    "        non_zero_mask_basic = val_labels_basic != 0\n",
    "        if np.any(non_zero_mask_basic):\n",
    "            mape_basic = np.mean(np.abs((val_labels_basic[non_zero_mask_basic] - val_predictions_basic[non_zero_mask_basic]) / val_labels_basic[non_zero_mask_basic])) * 100\n",
    "            print(f\"MAPE on Basic Validation Set (excluding zero labels): {mape_basic:.2f}%\")\n",
    "        else:\n",
    "            print(\"Warning: All basic validation labels are zero. MAPE is undefined.\")\n",
    "        if np.any(val_labels_basic == 0):\n",
    "             print(\"Note: Some basic validation labels were zero and excluded from MAPE calculation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during basic MAPE calculation: {e}\")\n",
    "else:\n",
    "    print(\"Basic validation data is empty or invalid. Skipping MAPE calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make Prediction on Testing Dataset (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_basic_final = processed_testing_datalist_basic_features[:, 0]\n",
    "output_datalist_basic = MakePrediction(w_basic, test_features_basic_final)\n",
    "print(\"Predictions on basic testing dataset (output_datalist_basic):\")\n",
    "print(output_datalist_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write Output File (Basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'output_datalist_basic' in globals() and 'output_dataroot_basic' in globals():\n",
    "    ids_basic = np.arange(1, len(output_datalist_basic) + 1)\n",
    "    output_array_basic = np.vstack((ids_basic, output_datalist_basic)).T\n",
    "    np.savetxt(output_dataroot_basic, output_array_basic, delimiter=',', header='Id,gripForce', fmt=['%d', '%.6f'], comments='')\n",
    "    print(f\"Basic predictions saved to {output_dataroot_basic}\")\n",
    "else:\n",
    "    print(\"Error: Basic output data not defined. Cannot save basic predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model and Generate Result (Advanced Part)\n",
    "\n",
    "This section handles multi-feature regression using matrix inversion, including a categorical feature (gender)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Advanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file_adv = 'lab1_advanced_training.csv'\n",
    "testing_file_adv = 'lab1_advanced_testing.csv'\n",
    "output_dataroot_adv = 'lab1_advanced.csv'\n",
    "\n",
    "try:\n",
    "    # Expecting columns: feature1, feature2, gender, gripForce\n",
    "    training_datalist_adv = np.genfromtxt(training_file_adv, delimiter=',', skip_header=1)\n",
    "    # Expecting columns: feature1, feature2, gender\n",
    "    testing_datalist_adv = np.genfromtxt(testing_file_adv, delimiter=',', skip_header=1)\n",
    "    print(f\"Successfully loaded {training_file_adv} and {testing_file_adv}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Advanced data files not found. Using dummy data for advanced part.\")\n",
    "    # Dummy training_datalist_adv (feature1, feature2, gender (0/1), gripForce)\n",
    "    training_datalist_adv = np.array([\n",
    "        [25, 170, 0, 30.5], [30, 165, 1, 25.2], [22, 175, 0, 35.0], [45, 155, 1, 22.8],\n",
    "        [28, 180, 0, 38.1], [35, 160, 1, 28.0], [40, 170, 0, 33.5], [20, 185, 0, 40.2],\n",
    "        [50, 150, 1, 20.0], [33, 172, 0, 32.8], [np.nan, 160, 1, 21.0], [30, 250, 0, 55.0] # Nan and outlier\n",
    "    ])\n",
    "    # Dummy testing_datalist_adv (feature1, feature2, gender (0/1))\n",
    "    testing_datalist_adv = np.array([\n",
    "        [26, 172, 0], [32, 168, 1], [24, 178, 0], [48, 158, 1], [30, 182, 0], [150, 160, 0] # Outlier\n",
    "    ])\n",
    "\n",
    "print(\"Initial advanced training_datalist shape:\", training_datalist_adv.shape)\n",
    "print(\"Initial advanced testing_datalist shape:\", testing_datalist_adv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess Data (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessDataAdvanced(data, gender_column_index=2):\n",
    "    \"\"\"\n",
    "    Preprocesses advanced dataset: handles NaNs and outliers for numerical columns,\n",
    "    excluding the gender column from outlier removal.\n",
    "    Assumes gender column is already numerically encoded (e.g., 0/1).\n",
    "    \"\"\"\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = np.array(data, dtype=float)\n",
    "    else:\n",
    "        data = data.astype(float)\n",
    "\n",
    "    # Handle NaNs by row removal (applied to all columns)\n",
    "    data_no_nan = data[~np.isnan(data).any(axis=1)]\n",
    "    if data_no_nan.size == 0:\n",
    "        print(\"Warning: All rows removed after NaN handling or data was initially empty.\")\n",
    "        return data_no_nan\n",
    "\n",
    "    # Separate numerical columns for outlier detection\n",
    "    # Create a mask for numerical columns to apply IQR\n",
    "    num_cols = data_no_nan.shape[1]\n",
    "    numerical_cols_mask = np.ones(num_cols, dtype=bool)\n",
    "    if 0 <= gender_column_index < num_cols:\n",
    "        numerical_cols_mask[gender_column_index] = False # Exclude gender column from IQR\n",
    "    \n",
    "    data_numerical_cols = data_no_nan[:, numerical_cols_mask]\n",
    "    \n",
    "    if data_numerical_cols.size > 0: # Proceed only if there are numerical columns to process\n",
    "        Q1 = np.percentile(data_numerical_cols, 25, axis=0)\n",
    "        Q3 = np.percentile(data_numerical_cols, 75, axis=0)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Create a boolean mask for non-outliers for numerical columns only\n",
    "        non_outlier_numerical_mask = np.all((data_numerical_cols >= lower_bound) & (data_numerical_cols <= upper_bound), axis=1)\n",
    "        \n",
    "        # Combine with original data (including gender column which was not checked for outliers)\n",
    "        preprocessed_data = data_no_nan[non_outlier_numerical_mask]\n",
    "    else: # No numerical columns to apply IQR to (e.g. only gender column or empty data)\n",
    "        preprocessed_data = data_no_nan\n",
    "\n",
    "    if preprocessed_data.size == 0 and data_no_nan.size > 0:\n",
    "        print(\"Warning: All rows removed after outlier handling.\")\n",
    "        \n",
    "    return preprocessed_data\n",
    "\n",
    "# Assuming gender is the 3rd column (index 2) in training_datalist_adv and testing_datalist_adv\n",
    "gender_col_idx_adv = 2 \n",
    "processed_training_adv = PreprocessDataAdvanced(training_datalist_adv, gender_column_index=gender_col_idx_adv)\n",
    "print(f\"Processed advanced training data shape: {processed_training_adv.shape}\")\n",
    "if processed_training_adv.size == 0:\n",
    "    raise ValueError(\"Advanced training data is empty after preprocessing.\")\n",
    "\n",
    "processed_testing_adv = PreprocessDataAdvanced(testing_datalist_adv, gender_column_index=gender_col_idx_adv)\n",
    "print(f\"Processed advanced testing data shape: {processed_testing_adv.shape}\")\n",
    "if processed_testing_adv.size == 0:\n",
    "    raise ValueError(\"Advanced testing data is empty after preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split Data (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio_adv = 0.8\n",
    "train_data_adv, val_data_adv = SplitData(processed_training_adv, split_ratio_adv)\n",
    "print(f\"Advanced training data shape: {train_data_adv.shape}\")\n",
    "print(f\"Advanced validation data shape: {val_data_adv.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Advanced Regression Model (Matrix Inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressionMatrixInversion(dataset):\n",
    "    \"\"\"\n",
    "    Performs multiple linear regression using matrix inversion (Normal Equation).\n",
    "    Assumes dataset has features first, and the last column is the target variable y.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset, np.ndarray) or dataset.ndim != 2 or dataset.shape[1] < 2:\n",
    "        raise ValueError(\"Dataset must be a 2D NumPy array with at least two columns (features + target).\")\n",
    "    \n",
    "    X = dataset[:, :-1]  # All columns except the last are features\n",
    "    y = dataset[:, -1]   # The last column is the target\n",
    "    \n",
    "    # Add intercept term (column of ones) to X\n",
    "    X_intercept = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "    \n",
    "    # Calculate weights using pseudo-inverse for numerical stability\n",
    "    # w = (X^T * X)^(-1) * X^T * y\n",
    "    try:\n",
    "        w = np.linalg.pinv(X_intercept.T @ X_intercept) @ X_intercept.T @ y\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Error: Singular matrix encountered. Cannot compute inverse directly. Pseudo-inverse also failed.\")\n",
    "        # Fallback or re-throw, here we'll let pinv potentially handle it or error out if it can't\n",
    "        raise \n",
    "    return w\n",
    "\n",
    "if train_data_adv.size > 0 and train_data_adv.shape[1] > 1:\n",
    "    print(\"Training advanced regression model using matrix inversion...\")\n",
    "    w_advanced = RegressionMatrixInversion(train_data_adv)\n",
    "    print(\"Learned weights for advanced model (w_advanced - intercept first):\")\n",
    "    print(w_advanced)\n",
    "else:\n",
    "    print(\"Advanced training data is empty or invalid. Skipping model training.\")\n",
    "    w_advanced = None # Ensure w_advanced exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict on Validation Set and Calculate MAPE (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePredictionAdvanced(w, test_features_dataset):\n",
    "    \"\"\"\n",
    "    Makes predictions using weights from matrix inversion regression.\n",
    "    test_features_dataset: NumPy array of features (no target column).\n",
    "    \"\"\"\n",
    "    if not isinstance(test_features_dataset, np.ndarray):\n",
    "        test_features_dataset = np.array(test_features_dataset, dtype=float)\n",
    "    if test_features_dataset.ndim == 1: # If a single sample with multiple features\n",
    "        test_features_dataset = test_features_dataset.reshape(1, -1)\n",
    "    \n",
    "    # Add intercept term\n",
    "    X_intercept_test = np.concatenate((np.ones((test_features_dataset.shape[0], 1)), test_features_dataset), axis=1)\n",
    "    \n",
    "    predictions = X_intercept_test @ w\n",
    "    return predictions\n",
    "\n",
    "if w_advanced is not None and val_data_adv.size > 0 and val_data_adv.shape[1] > 1:\n",
    "    val_features_adv = val_data_adv[:, :-1] # All columns except last\n",
    "    val_labels_adv = val_data_adv[:, -1]    # Last column\n",
    "    \n",
    "    val_predictions_adv = MakePredictionAdvanced(w_advanced, val_features_adv)\n",
    "\n",
    "    try:\n",
    "        non_zero_mask_adv = val_labels_adv != 0\n",
    "        if np.any(non_zero_mask_adv):\n",
    "            mape_adv = np.mean(np.abs((val_labels_adv[non_zero_mask_adv] - val_predictions_adv[non_zero_mask_adv]) / val_labels_adv[non_zero_mask_adv])) * 100\n",
    "            print(f\"MAPE on Advanced Validation Set (excluding zero labels): {mape_adv:.2f}%\")\n",
    "        else:\n",
    "            print(\"Warning: All advanced validation labels are zero. MAPE is undefined.\")\n",
    "        if np.any(val_labels_adv == 0):\n",
    "            print(\"Note: Some advanced validation labels were zero and excluded from MAPE calculation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during advanced MAPE calculation: {e}\")\n",
    "else:\n",
    "    print(\"Advanced validation data/weights invalid. Skipping MAPE calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make Prediction on Advanced Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if w_advanced is not None and processed_testing_adv.size > 0:\n",
    "    # processed_testing_adv contains only features, as per its loading and preprocessing\n",
    "    output_datalist_advanced = MakePredictionAdvanced(w_advanced, processed_testing_adv)\n",
    "    print(\"Predictions on advanced testing dataset (output_datalist_advanced):\")\n",
    "    print(output_datalist_advanced)\n",
    "else:\n",
    "    print(\"Advanced testing data or weights invalid. Cannot make predictions.\")\n",
    "    output_datalist_advanced = np.array([]) # Ensure variable exists for saving step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write Output File (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'output_datalist_advanced' in globals() and output_datalist_advanced.size > 0 and 'output_dataroot_adv' in globals():\n",
    "    ids_adv = np.arange(1, len(output_datalist_advanced) + 1)\n",
    "    output_array_adv = np.vstack((ids_adv, output_datalist_advanced)).T\n",
    "    np.savetxt(output_dataroot_adv, output_array_adv, delimiter=',', header='Id,gripForce', fmt=['%d', '%.6f'], comments='')\n",
    "    print(f\"Advanced predictions saved to {output_dataroot_adv}\")\n",
    "elif 'output_datalist_advanced' in globals() and output_datalist_advanced.size == 0:\n",
    "    print(\"No advanced predictions to save (output_datalist_advanced is empty).\")\n",
    "else:\n",
    "    print(\"Error: Advanced output data not defined. Cannot save advanced predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
