{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math # Added for math.floor\n",
    "import matplotlib.pyplot as plt # Added for plotting\n",
    "import pandas as pd # Added for DataFrame\n",
    "# Assuming dummy files are in the same directory or path is configured\n",
    "from Loss import compute_BCE_loss # For compute_BCE_loss\n",
    "from Predict import predict # For predict function\n",
    "# from Dense import Dense # Not strictly needed for this cell if Model uses it internally\n",
    "# from Activation import Activation # Not strictly needed for this cell if Model uses it internally\n",
    "\n",
    "output = {} # Initialize the output dictionary globally\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad all images in the dataset X with zeros.\n",
    "    The padding should be applied to both the height and width of each image.\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C), where m represent the number of examples.\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant', constant_values = (0,0))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of previous activation layer output with shape (filter_size, filter_size, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (filter_size, filter_size, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Element-wise product to a_slice_prev and W\n",
    "    s = a_slice_prev * W\n",
    "    # Step 2: Sum all values to get a single scalar\n",
    "    Z = np.sum(s)\n",
    "    # Step 3: Add the bias\n",
    "    Z = Z + float(b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv():\n",
    "    def __init__(self, filter_size=2, input_channel=3, output_channel=8, pad=1, stride=1, seed=1):\n",
    "        self.filter_size = filter_size\n",
    "        self.input_channel = input_channel\n",
    "        self.output_channel = output_channel\n",
    "        self.seed = seed # seed is used in initialize_parameters\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "        self.parameters = {'W':None, 'b': None}\n",
    "        self.initialize_parameters()\n",
    "        # Initialize dW and db as well, as they are used in update and set in backward\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.cache = None # cache is used in backward and set in forward\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(self.seed) # Uses self.seed\n",
    "        sd = np.sqrt(6.0 / (self.input_channel * self.filter_size * self.filter_size + self.output_channel * self.filter_size * self.filter_size)) # Corrected Xavier/He init for conv\n",
    "        W = np.random.uniform(-sd, sd, (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n",
    "        # W = np.random.randn(self.filter_size,self.filter_size,self.input_channel,self.output_channel) * np.sqrt(2./(self.filter_size*self.filter_size*self.input_channel)) # Alternative He init\n",
    "        b = np.zeros((1, 1, 1, self.output_channel))\n",
    "        assert(W.shape == (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n",
    "        assert(b.shape == (1,1,1,self.output_channel))\n",
    "        self.parameters['W'] = W\n",
    "        self.parameters['b'] = b\n",
    "\n",
    "Conv.conv_single_step = conv_single_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, A_prev):\n",
    "    \"\"\" Implements the forward propagation for a convolution layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    # W has shape (filter_size, filter_size, input_channel, output_channel)\n",
    "    # self.parameters['W'] has shape (f, f, n_C_prev_from_W, n_C_from_W)\n",
    "    (f, _, n_C_prev_from_W, n_C) = self.parameters['W'].shape \n",
    "    assert n_C_prev == n_C_prev_from_W, f\"Number of input channels in A_prev ({n_C_prev}) and W ({n_C_prev_from_W}) must match\"\n",
    "    assert f == self.filter_size, f\"Filter size in W ({f}) and self.filter_size ({self.filter_size}) must match\"\n",
    "    assert n_C == self.output_channel, f\"Output channels in W ({n_C}) and self.output_channel ({self.output_channel}) must match\"\n",
    "\n",
    "    # Step 1: Output Dimension Calculation\n",
    "    pad = self.pad\n",
    "    stride = self.stride\n",
    "    n_H = int(np.floor((n_H_prev - f + 2 * pad) / stride)) + 1\n",
    "    n_W = int(np.floor((n_W_prev - f + 2 * pad) / stride)) + 1\n",
    "\n",
    "    # Initialize the output volume Z with zeros\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    # Step 2: Padding\n",
    "    A_prev_pad = zero_pad(A_prev, pad) # use zero_pad function\n",
    "\n",
    "    # Step 3: Loop Through Training Examples\n",
    "    for i in range(m): # loop over the batch of training examples\n",
    "        a_prev_pad_example = A_prev_pad[i] # Select the i-th example's padded activation\n",
    "        for h in range(n_H): # loop over vertical axis of the output volume\n",
    "            for w in range(n_W): # loop over horizontal axis of the output volume\n",
    "                for c_out in range(n_C): # loop over channels (= #filter) of the output volume\n",
    "\n",
    "                    # Step 3-1: Extracting slices\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    a_slice_prev = a_prev_pad_example[vert_start:vert_end, horiz_start:horiz_end, :] # Slice all input channels for this slice\n",
    "\n",
    "                    # Step 3-2: Applying Filters\n",
    "                    # Use self.conv_single_step(), parameters W and b for the c_out-th filter\n",
    "                    weights = self.parameters['W'][:,:,:,c_out]\n",
    "                    biases = self.parameters['b'][:,:,:,c_out]\n",
    "                    Z[i, h, w, c_out] = self.conv_single_step(a_slice_prev, weights, biases)\n",
    "    ### END CODE HERE ###\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    # Save information in \"cache\" for the backward pass\n",
    "    self.cache = A_prev\n",
    "\n",
    "    return Z\n",
    "\n",
    "Conv.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dZ):\n",
    "    \"\"\" Implement the backward propagation for a convolution layer\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev = self.cache\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, _, _, n_C) = self.parameters['W'].shape # W has shape (f, f, n_C_prev, n_C)\n",
    "\n",
    "    # Retrieve stride and pad from self\n",
    "    stride = self.stride\n",
    "    pad = self.pad\n",
    "\n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m_dZ, n_H, n_W, n_C_dZ) = dZ.shape # n_C here is the number of filters, should match n_C from W\n",
    "    assert n_C == n_C_dZ, f\"Number of channels in dZ ({n_C_dZ}) must match output channels in W ({n_C})\"\n",
    "    \n",
    "    # Initialize Gradients\n",
    "    dA_prev = np.zeros_like(A_prev) # Shape: (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW = np.zeros_like(self.parameters['W']) # Shape: (f, f, n_C_prev, n_C)\n",
    "    db = np.zeros_like(self.parameters['b']) # Shape: (1, 1, 1, n_C)\n",
    "\n",
    "    # Padding A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad) # Pad dA_prev for accumulating gradients\n",
    "\n",
    "    # Loop Through Training Examples\n",
    "    for i in range(m):                         # loop over the batch of training examples\n",
    "        # Select ith training example's padded activation\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        # Select ith training example's padded dA_prev (this will be updated)\n",
    "        da_prev_pad_example = dA_prev_pad[i] \n",
    "\n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c_filter in range(n_C):           # loop over the channels of the output volume (number of filters)\n",
    "\n",
    "                    # Extracting Slices\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] # shape (f, f, n_C_prev)\n",
    "\n",
    "                    # Update Gradients\n",
    "                    # dZ[i, h, w, c_filter] is a scalar\n",
    "                    # self.parameters['W'][:,:,:,c_filter] has shape (f, f, n_C_prev)\n",
    "                    da_prev_pad_example[vert_start:vert_end, horiz_start:horiz_end, :] += self.parameters['W'][:,:,:,c_filter] * dZ[i, h, w, c_filter]\n",
    "                    dW[:,:,:,c_filter] += a_slice * dZ[i, h, w, c_filter]\n",
    "                    db[:,:,:,c_filter] += dZ[i, h, w, c_filter]\n",
    "        \n",
    "        # After processing all h, w, c_filter for the current example i, update the main dA_prev_pad array\n",
    "        # This line was dA_prev_pad[i] = da_prev_pad_example, which is redundant as da_prev_pad_example IS dA_prev_pad[i]\n",
    "        # No explicit assignment needed here as da_prev_pad_example is a reference to dA_prev_pad[i]'s content\n",
    "\n",
    "        # Remove Padding from dA_prev_pad for the current example and store in dA_prev\n",
    "        if pad > 0:\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
    "        else:\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad[i, :, :, :] # No padding to remove\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    # Store gradients in self\n",
    "    self.dW = dW\n",
    "    self.db = db\n",
    "\n",
    "    return dA_prev\n",
    "\n",
    "Conv.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, learning_rate):\n",
    "    \"\"\" Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    learning_rate -- step size\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "    self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "Conv.update = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool():\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def create_mask_from_window(self, x):\n",
    "        \"\"\" Creates a mask from an input matrix x, to identify the max entry.\n",
    "\n",
    "        Arguments:\n",
    "        x -- Array of shape (pool_size, pool_size)\n",
    "\n",
    "        Returns:\n",
    "        mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        mask = (x == np.max(x))\n",
    "        ### END CODE HERE ###\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, A_prev):\n",
    "    \"\"\" Implements the forward pass of the max pooling layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve pool_size and stride from self\n",
    "    pool_size = self.pool_size\n",
    "    stride = self.stride\n",
    "\n",
    "    # Step 1: Output Dimension Calculation\n",
    "    n_H = int(np.floor((n_H_prev - pool_size) / stride)) + 1\n",
    "    n_W = int(np.floor((n_W_prev - pool_size) / stride)) + 1\n",
    "    n_C = n_C_prev # n_C is the same as n_C_prev\n",
    "\n",
    "    # initialize output matrix A with zeros\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    # Step 2: Loop Through Training Examples\n",
    "    for i in range(m):                           # loop over the batch of training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                    # Step 2-1: Extracting slices\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + pool_size\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + pool_size\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                    # Step 2-2: Applying Maxpooling\n",
    "                    A[i, h, w, c] = np.max(a_prev_slice)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Store the input in \"cache\" for backward pass\n",
    "    self.cache = A_prev\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    return A\n",
    "\n",
    "MaxPool.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dA):\n",
    "    \"\"\" Implements the backward pass of the max pooling layer\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve information from cache\n",
    "    A_prev = self.cache\n",
    "    pool_size = self.pool_size # Added\n",
    "    stride = self.stride       # Added\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape # Corrected\n",
    "    (m_dA, n_H, n_W, n_C) = dA.shape # Corrected variable names, m should be same, n_C should be same as n_C_prev\n",
    "    assert m == m_dA, \"Number of examples in A_prev and dA must be the same\"\n",
    "    assert n_C_prev == n_C, \"Number of channels in A_prev and dA must be the same\"\n",
    "\n",
    "    # Step 1: Initialize Gradients\n",
    "    dA_prev = np.zeros_like(A_prev) # Corrected\n",
    "\n",
    "    # Step 2: Loop Through Training Examples\n",
    "    for i in range(m_dA): # loop over the batch of training examples, use m from dA\n",
    "        a_prev_example = A_prev[i] # Select the i-th example from A_prev\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c_loop in range (n_C):            # loop over the channels of the output volume (use c_loop to avoid conflict with n_C)\n",
    "\n",
    "                    # Step 2-1: Extracting slices from a_prev_example\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + pool_size\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + pool_size\n",
    "                    a_prev_slice = a_prev_example[vert_start:vert_end, horiz_start:horiz_end, c_loop]\n",
    "\n",
    "                    # Step 2-2: Pass through the Gradients\n",
    "                    mask = self.create_mask_from_window(a_prev_slice) # Use self.create_mask_from_window\n",
    "                    dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c_loop] += mask * dA[i, h, w, c_loop]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Make sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "    return dA_prev\n",
    "\n",
    "MaxPool.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        self.cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, A_prev):\n",
    "    \"\"\" Implements the forward pass of the flatten layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "    Returns:\n",
    "    A -- output of the flatten layer, a 2-dimensional array of shape (m, (n_H_prev * n_W_prev * n_C_prev))\n",
    "    \"\"\"\n",
    "\n",
    "    # Save information in \"cache\" for the backward pass\n",
    "    self.cache = A_prev.shape\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    A = A_prev.reshape(A_prev.shape[0], -1)\n",
    "    ### END CODE HERE ###\n",
    "    return A\n",
    "\n",
    "Flatten.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dA):\n",
    "    \"\"\" Implements the backward pass of the flatten layer\n",
    "\n",
    "    Arguments:\n",
    "    dA -- Input data, a 2-dimensional array\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- An array with its original shape (the output shape of its' previous layer).\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    dA_prev = dA.reshape(self.cache)\n",
    "    ### END CODE HERE ###\n",
    "    return dA_prev\n",
    "\n",
    "Flatten.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    A = X\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    for l in range(len(self.layers)):\n",
    "        current_layer = self.layers[l] # Get current layer\n",
    "        A = current_layer.forward(A)   # Call forward method of the current layer\n",
    "    ### END CODE HERE ###\n",
    "    return A\n",
    "\n",
    "Model.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, AL=None, Y=None):\n",
    "    L = len(self.layers)\n",
    "    # Y = Y.reshape(AL.shape) # Ensure Y and AL have the same shape if not already\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Initial gradient calculation for the output layer (dAL)\n",
    "    # This is dL/dAL, where L is the BCE loss\n",
    "    # Ensure Y has the same shape as AL for element-wise operations\n",
    "    if Y.shape != AL.shape:\n",
    "        Y = Y.reshape(AL.shape)\n",
    "        \n",
    "    dAL = - (np.divide(Y, AL, out=np.zeros_like(AL), where=AL!=0) - \\\n",
    "             np.divide(1 - Y, 1 - AL, out=np.zeros_like(AL), where=(1-AL)!=0))\n",
    "\n",
    "\n",
    "    # Backward pass for the last layer\n",
    "    current_layer = self.layers[L-1]\n",
    "    # For an Activation layer (e.g., sigmoid), its backward method takes dAL (dL/d(output of activation))\n",
    "    # and computes dZ (dL/d(input of activation), which is output of previous Dense layer for example)\n",
    "    dA_prev = current_layer.backward(dAL) # dA_prev here is actually dZ for the last activation layer, or dA for a layer like Flatten if it's last.\n",
    "\n",
    "    # Loop from l=L-2 to l=0 (i.e., second to last layer down to the first layer)\n",
    "    for l in reversed(range(L-1)): # L-1 is the index of the last layer, so L-2 is the one before it\n",
    "        current_layer = self.layers[l]\n",
    "        # The dA_prev from the (l+1)-th layer's backward pass becomes the dA for the l-th layer's backward pass\n",
    "        dA_prev = current_layer.backward(dA_prev)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dA_prev\n",
    "\n",
    "Model.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, learning_rate):\n",
    "\n",
    "    # Only convolution layer and dense layer have to update parameters\n",
    "    ### START CODE HERE ###\n",
    "    for l in range(len(self.layers)):\n",
    "      current_layer = self.layers[l]\n",
    "      if hasattr(current_layer, 'update'): # Check if the layer has an update method\n",
    "          current_layer.update(learning_rate)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "Model.update = update"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Application in a real case\n",
    "\n",
    "Now, you have all the necessary functions to build your own ConvNet. In this final part, you will get to implement a ConvNet that classifies images from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the .npz file\n",
    "### START CODE HERE ###\n",
    "# Note: Replace 'Lab5_data.npz' with the actual filename if it's different.\n",
    "# Note: Adjust the keys ('X_train', 'y_train', 'X_test') if they are different in your .npz file.\n",
    "data = np.load('Lab5_data.npz') \n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test'] # Or use the correct key for test images, e.g., 'X_test_images'\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Example: Print shapes to verify\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "# print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, n_H, n_W, n_C) for images\n",
    "    Y -- true \"label\" vector, of shape (number of examples, 1)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "    # np.random.seed(seed) # Seed is usually set globally if needed for reproducibility\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k * mini_batch_size) + mini_batch_size, :,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k * mini_batch_size) + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming Dense and Activation are imported if they are in separate .py files\n",
    "# from Dense import Dense\n",
    "# from Activation import Activation\n",
    "\n",
    "### START CODE HERE ###\n",
    "learning_rate = 3e-5\n",
    "num_iterations = 20 \n",
    "batch_size = 64    \n",
    "costs = []         \n",
    "\n",
    "model = Model()\n",
    "# Input images are 32x32x1 (as per problem description)\n",
    "model.add(Conv(filter_size=3, input_channel=1, output_channel=8, pad=1, stride=1, seed=1))\n",
    "model.add(Activation(\"relu\", None))\n",
    "model.add(MaxPool(pool_size=2, stride=2))\n",
    "# After MaxPool (2x2, stride 2) on 32x32, output is 16x16x8\n",
    "# model.add(Conv(filter_size=3, input_channel=8, output_channel=16, pad=1, stride=1, seed=2))\n",
    "# model.add(Activation(\"relu\", None))\n",
    "# model.add(MaxPool(pool_size=2, stride=2))\n",
    "# After second MaxPool (2x2, stride 2) on 16x16, output is 8x8x16\n",
    "model.add(Flatten()) \n",
    "# Flattened output: 16*16*8 = 2048 if one Conv block\n",
    "# Flattened output: 8*8*16 = 1024 if two Conv blocks\n",
    "# The Dense layer's input_size must match this. The Dense class from Lab4 might infer this.\n",
    "# We specify number of units in this dense layer (e.g. 128) and output units (1 for sigmoid).\n",
    "model.add(Dense(128, 1)) # 128 units in this Dense layer, 1 output unit for sigmoid\n",
    "model.add(Activation(\"sigmoid\", None))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for i in range(num_iterations):\n",
    "    print(\"epoch: \",i)\n",
    "    # Ensure X_train and y_train are available from the data loading cell (4.1)\n",
    "    # And that y_train is in the correct shape, e.g. (m, 1) or (1, m) as expected by compute_BCE_loss and model.backward\n",
    "    # The random_mini_batches expects Y of shape (m, 1)\n",
    "    # compute_BCE_loss expects Y and AL of shape (1, m) or (m, 1) if consistent.\n",
    "    # Our Model.backward expects Y and AL to be (m, n_classes) or (1,m) for BCE. Let's assume (1,m) for Y to match typical BCE.\n",
    "    # If y_train is (m,1), it needs to be reshaped to (1,m) for compute_BCE_loss and Model.backward's dAL calculation.\n",
    "    # However, random_mini_batches returns y_batch as (batch_size, 1). So AL from model.forward(x_batch) should also be (batch_size, 1).\n",
    "    # Let's adjust compute_BCE_loss in Loss.py to handle Y (batch_size,1) and AL (batch_size,1) by transposing them before np.dot if needed.\n",
    "    # Or, ensure AL from Dense layer is (batch_size, 1) and Y is (batch_size, 1).\n",
    "    # The current Loss.py expects (1,m). The Dense layer from lab4 likely produces (output_units, m).\n",
    "    # For consistency, we assume model.forward returns (num_classes, m) and Y is (num_classes, m).\n",
    "    # If y_train loaded is (m,), reshape to (1,m). If (m,1) also reshape to (1,m).\n",
    "    # For this lab, let's assume y_train is (m,1) and model output AL is (m,1) after Dense(...,1). Then Loss function needs to handle this.\n",
    "    # The provided Loss.py is: cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T)). Y and AL (1,m)\n",
    "    # If AL is (m,1) and Y_batch is (m,1) from random_mini_batches, then this is - (1/m) * sum(Y*log(AL) + (1-Y)*log(1-AL))\n",
    "    # Let's assume random_mini_batches returns y_batch as (mini_batch_size, 1) and AL from model as (mini_batch_size, 1)\n",
    "    # And that compute_BCE_loss is adjusted or expects this. (The dummy one expects (1,m))\n",
    "    # For simplicity, let's make sure y_batch and AL are (1, mini_batch_size) for the loss function.\n",
    "\n",
    "    mini_batches = random_mini_batches(X_train, y_train, batch_size)\n",
    "    epoch_cost = 0. # Cost for the current epoch, can be sum or average over batches\n",
    "    num_batches = len(mini_batches)\n",
    "\n",
    "    for batch in mini_batches:\n",
    "        (x_batch, y_batch) = batch # x_batch (bs,H,W,C), y_batch (bs,1)\n",
    "\n",
    "        # Forward pass\n",
    "        AL = model.forward(x_batch) # Expect AL to be (bs, 1) from Dense(...,1) and Sigmoid\n",
    "\n",
    "        # Compute cost\n",
    "        # Ensure y_batch and AL are compatible with compute_BCE_loss\n",
    "        # compute_BCE_loss expects (1,m). AL might be (bs,1) or (1,bs). y_batch is (bs,1).\n",
    "        # Reshape y_batch to (1, bs) and AL to (1, bs) if it's (bs,1)\n",
    "        y_batch_reshaped = y_batch.T # from (bs,1) to (1,bs)\n",
    "        AL_reshaped = AL.T       # from (bs,1) to (1,bs) if Dense output is (bs, units)\n",
    "                                 # If Dense output is (units, bs) then AL is already (1,bs)\n",
    "                                 # Let's assume model.forward() for Dense layer returns (units, m) as per Lab4\n",
    "                                 # So AL here would be (1, batch_size)\n",
    "        current_batch_cost = compute_BCE_loss(y_batch_reshaped, AL) # AL is assumed (1, bs)\n",
    "        epoch_cost += current_batch_cost\n",
    "\n",
    "        # Backward pass\n",
    "        dA_prev = model.backward(AL, y_batch_reshaped) # model.backward expects AL (1,bs), Y (1,bs)\n",
    "\n",
    "        # Update parameters\n",
    "        model.update(learning_rate)\n",
    "    \n",
    "    # Print the cost (average cost over batches for the epoch)\n",
    "    epoch_avg_cost = epoch_cost / num_batches\n",
    "    print (\"Cost after iteration %i: %f\" %(i, epoch_avg_cost))\n",
    "    costs.append(epoch_avg_cost)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Plot the cost\n",
    "plt.plot(np.squeeze(costs)) # costs is a list of scalar epoch_avg_cost, squeeze is robust\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs') # Changed from 'iterations' to 'epochs' for clarity as num_iterations means epochs\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on training data\n",
    "print('training------')\n",
    "# predict function from Predict.py is expected to print accuracy if y is provided\n",
    "pred_train = predict(model, X_train, y_train) \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5 Generate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model and X_test are available from previous steps\n",
    "# X_test should be loaded in section 4.1\n",
    "# model should be trained in section 4.3\n",
    "\n",
    "### START CODE HERE ###\n",
    "pred_test = predict(model, X_test) # predict function from Predict.py\n",
    "\n",
    "# Ensure pred_test is in a suitable format (e.g., (N, 1) or (N,))\n",
    "# and contains 0s or 1s.\n",
    "# The dummy predict function returns p as (1,m) with integer predictions.\n",
    "# So pred_test.astype(int).flatten() should work fine.\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'ID': range(len(X_test)),  # Assumes X_test is loaded and len(X_test) gives num samples\n",
    "    'Label': pred_test.astype(int).flatten() # Cast to int and flatten\n",
    "})\n",
    "df.to_csv('Lab5_prediction.csv', index=False, mode='w')\n",
    "\n",
    "print(\"Lab5_prediction.csv generated.\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
    "cell_type": "markdown",
    "source": [
     "## 5. Generate Lab5_output.npy"
    ],
    "metadata": {}
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Ensure the 'output' dictionary has been populated by previous \"Test and Evaluate\" cells.\n",
     "# This script part just saves it and does a sanity check.\n",
     "\n",
     "### START CODE HERE ###\n",
     "np.save(\"Lab5_output.npy\", output) # output should be the dictionary\n",
     "\n",
     "# sanity check\n",
     "submit = np.load(\"Lab5_output.npy\", allow_pickle=True).item()\n",
     "for key, value in submit.items():\n",
     "    print(str(key) + \": \" + str(type(value)))\n",
     "\n",
     "# Expected keys for the assertion:\n",
     "expected_keys = [\n",
     "    'zero_padding', 'conv_single_step', 'conv_forward_1', 'conv_forward_2', \n",
     "    'conv_forward_3','conv_backward_1', 'conv_backward_2', 'conv_backward_3', \n",
     "    'conv_update_1', 'conv_update_2', 'maxpool_forward', 'maxpool_backward', \n",
     "    'flatten_forward', 'flatten_backward', 'model_1', 'model_2', 'model_3', 'model_4'\n",
     "]\n",
     "assert(list(output.keys()) == expected_keys)\n",
     "\n",
     "print(\"\\nLab5_output.npy generated and sanity check passed.\")\n",
     "### END CODE HERE ###"
    ]
   }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
